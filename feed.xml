<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://dsl-lab.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dsl-lab.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-07T18:06:46+00:00</updated><id>https://dsl-lab.github.io/feed.xml</id><title type="html">Laboratory for Deep Structured Learning</title><subtitle>Renjie Liao&apos;s Lab for deep structured learning. </subtitle><entry><title type="html">Evaluating Motion Consistency by Fréchet Video Motion Distance (FVMD)</title><link href="https://dsl-lab.github.io/blog/2024/fvmd-2/" rel="alternate" type="text/html" title="Evaluating Motion Consistency by Fréchet Video Motion Distance (FVMD)"/><published>2024-06-30T00:00:00+00:00</published><updated>2024-06-30T00:00:00+00:00</updated><id>https://dsl-lab.github.io/blog/2024/fvmd-2</id><content type="html" xml:base="https://dsl-lab.github.io/blog/2024/fvmd-2/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Recently, diffusion models have demonstrated remarkable capabilities in high-quality image generation. This advancement has been extended to the video domain, giving rise to text-to-video diffusion models, such as <a href="https://pika.art/home">Pika</a>, <a href="https://research.runwayml.com/gen2">Runway Gen-2</a>, and <a href="https://openai.com/index/sora/">Sora</a> <d-cite key="videoworldsimulators2024"></d-cite>.</p> <p>Despite the rapid development of video generation models, research on evaluation metrics for video generation remains insufficient (see more discussion on our <a href="https://dsl-lab.github.io/blog/2024/fvmd-1/">blog</a>). For example, FID-VID <d-cite key="balaji2019conditional"></d-cite> and FVD <d-cite key="unterthiner2018towards"></d-cite> are commonly used video metrics. FID-VID focuses on visual quality by comparing synthesized <em>frames</em> to real ones, ignoring motion quality. FVD adds temporal coherence by using features from a <em>pre-trained action recognition model</em>, Inflated 3D Convnet (I3D) <d-cite key="carreira2017quo"></d-cite>. Recently, VBench <d-cite key="huang2023vbench"></d-cite> introduces a 16-dimensional evaluation suite for text-to-video generative models. However, VBench’s protocols for temporal consistency, like temporal flickering and motion smoothness, favor videos with smooth or static movement, <em>neglecting high-quality videos with intense motion</em>, such as dancing and sports videos.</p> <p>Simply put, there is a lack of metrics <strong>specifically designed to evaluate the complex motion patterns in generated videos</strong>. The Fréchet Video Motion Distance (FVMD) addresses this gap.</p> <p>The code is available at <a href="https://github.com/DSL-Lab/FVMD-frechet-video-motion-distance">GitHub</a>.</p> <h2 id="fréchet-video-motion-distance-fvmd">Fréchet Video Motion Distance (FVMD)</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/pipeline-480.webp 480w,/blog/2024/fvmd/pipeline-800.webp 800w,/blog/2024/fvmd/pipeline-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The overall pipeline of the Fréchet Video Motion Distance (FVMD) that measures the discrepancy in motion features between generated videos and ground-truth videos. </div> <p>The core idea of FVMD is to measure temporal motion consistency based on <strong>the patterns of velocity and acceleration in video movements</strong>. First, motion trajectories of key points are extracted using the pre-trained model PIPs++ <d-cite key="zheng2023pointodyssey"></d-cite>, and their velocity and acceleration are computed across frames. Motion features are then derived from the statistics of these vectors. Finally, the similarity between the motion features of generated and ground truth videos is measured using the Fréchet distance <d-cite key="dowson1982frechet"></d-cite>.</p> <h3 id="video-key-points-tracking">Video Key Points Tracking</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/tracking_demo_1-480.webp 480w,/blog/2024/fvmd/tracking_demo_1-800.webp 800w,/blog/2024/fvmd/tracking_demo_1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/tracking_demo_1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/tracking_demo_2-480.webp 480w,/blog/2024/fvmd/tracking_demo_2-800.webp 800w,/blog/2024/fvmd/tracking_demo_2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/tracking_demo_2.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Key point tracking results on the TikTok datasets <d-cite key="jafarian2022self"></d-cite> using PIPs++ <d-cite key="zheng2023pointodyssey"></d-cite>. </div> <p>To construct video motion features, key point trajectories are first tracked across the video sequence using PIPs++. For a set of $m$ generated videos, denoted as $\lbrace X^{(i)} \rbrace_{i=1}^m$, the tracking process begins by truncating longer videos into segments of $F$ frames with an overlap stride of $s$. For simplicity, segments from different videos are put together to form a single dataset $\lbrace x_{i} \rbrace_{i=1}^n$. Then, $N$ evenly-distributed target points in a grid shape are queried on the initial frames <d-footnote> For example, $F=16, s=1, N=400$ are used as default parameters to extract consecutive short segments.</d-footnote> and their trajectories are estimated across the video segments, resulting in a tensor $\hat{Y} \in \mathbb{R}^{F \times N \times 2}$.</p> <h3 id="key-points-velocity-and-acceleration-fields">Key Points Velocity and Acceleration Fields</h3> <p>FVMD proposes using the velocity and acceleration fields across frames to represent video motion patterns. The <strong>velocity field</strong> $\hat{V} \in \mathbb{R}^{F \times N \times 2}$ measures the first-order difference in key point positions between consecutive frames with zero-padding:</p> \[\hat{V} = \texttt{concat}(\boldsymbol{0}_{N\times 2}, \hat{Y}_{2:F} - \hat{Y}_{1:F-1}) \in \mathbb{R}^{F \times N \times 2},\] <p>The <strong>acceleration field</strong> $\hat{A} \in \mathbb{R}^{F \times N \times 2}$ is calculated by taking the first-order difference between the velocity fields in two consecutive frames, also with zero-padding:</p> \[\hat{A} = \texttt{concat}(\boldsymbol{0}_{N\times 2}, \hat{V}_{2:F} - \hat{V}_{1:F-1}) \in \mathbb{R}^{F \times N \times 2}.\] <h3 id="motion-feature">Motion Feature</h3> <p>To obtain compact motion features, the velocity and acceleration fields are further processed into <strong>spatial and temporal statistical histograms</strong>.</p> <p>First, the <em>magnitude and angle</em> for each tracking point in the velocity and acceleration vector fields are computed respectively. Let $\rho(U)$ and $\phi(U)$ denote the magnitude and angle of a vector field $U$, where $U \in \mathbb{R}^{F \times N \times 2}$ and $U$ can be either $\hat{V}$ or $\hat{A}$.</p> \[\begin{aligned} \rho(U)_{i, j} &amp;= \sqrt{U_{i,j,1}^2 + U_{i,j,2}^2}, &amp;\forall i \in [F], j \in [N], \\ \phi(U)_{i, j} &amp;= \left| \tanh^{-1}\left(\frac{U_{i, j,1}}{U_{i, j,2}}\right) \right|, &amp;\forall i \in [F], j \in [N]. \end{aligned}\] <p>Then, FVMD quantizes magnitudes and angles into discrete bins (8 for angles and 9 for magnitudes), which are then used to construct statistical histograms and extract motion features. It adopts <strong>dense 1D histograms</strong> <d-footnote>The 1D histogram approach is inspired by the HOG (Histogram of Oriented Gradients) approach <d-cite key="dalal2005histograms"></d-cite>, which counts occurrences of gradient orientation in localized portions of an image.</d-footnote> by aggregating magnitude values into 1D histograms corresponding to the quantized angles. Specifically, the $F$-frame video segments are divided into smaller volumes of size $f \times k \times k$, where $f$ is the number of frames and $k$ the number of tracking points. Within each small volume, every tracking point’s magnitude is summed into its corresponding angle bin, resulting in an 8-point histogram per volume. Eventually, the histograms from all volumes are combined to form the final motion feature <d-footnote>The shape of the dense 1D histogram is $ \lfloor \frac{F}{f} \rfloor \times \lfloor \frac{\sqrt{N}}{k} \rfloor \times \lfloor \frac{\sqrt{N}}{k} \rfloor \times 8$.</d-footnote>.</p> <p>Dense 1D histograms are used for <strong>both velocity and acceleration fields</strong>, and the resulting features are concatenated to form a combined motion feature for computing similarity. </p> <details> <summary>click here for 2D histogram construction</summary> FVMD also explores quantized 2D histograms but opts to use the dense 1D histograms for the default configuration due to their superior performance. In this approach, the corresponding vector fields of each volume are aggregated to form a 2D histogram, where $x$ and $y$ coordinates represent magnitudes and angles, respectively. The 2D histograms from all volumes are then concatenated and flattened into a vector to serve as the motion feature. The shape of the quantized 2D histogram is $ \lfloor \frac{F}{f} \rfloor \times \lfloor \frac{\sqrt{N}}{k} \rfloor \times \lfloor \frac{\sqrt{N}}{k} \rfloor \times 72$, where the number 72 is derived from 8 discrete bins for angle and 9 bins for magnitude. </details> <h3 id="visualizations">Visualizations</h3> <p>If two videos are of very different quality, their histograms should look very <em>different</em> to serve as a discriminative motion feature. Let’s take a look at what they look like for the videos in real life.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/gt-480.webp 480w,/blog/2024/fvmd/gt-800.webp 800w,/blog/2024/fvmd/gt-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/gt.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/disco-480.webp 480w,/blog/2024/fvmd/disco-800.webp 800w,/blog/2024/fvmd/disco-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/disco.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/anyone-480.webp 480w,/blog/2024/fvmd/anyone-800.webp 800w,/blog/2024/fvmd/anyone-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/anyone.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/gt_tracking-480.webp 480w,/blog/2024/fvmd/gt_tracking-800.webp 800w,/blog/2024/fvmd/gt_tracking-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/gt_tracking.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/disco_tracking-480.webp 480w,/blog/2024/fvmd/disco_tracking-800.webp 800w,/blog/2024/fvmd/disco_tracking-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/disco_tracking.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/anyone_tracking-480.webp 480w,/blog/2024/fvmd/anyone_tracking-800.webp 800w,/blog/2024/fvmd/anyone_tracking-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/anyone_tracking.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Raw videos and tracking results on the TikTok datasets <d-cite key="jafarian2022self"></d-cite>. Left: Ground-truth video. Middle and right: Generated videos for the same scene of worse (middle) and better (right) quality, respectively. </div> <p>Above, we show three pieces of video from the TikTok dataset <d-cite key="jafarian2022self"></d-cite> with very different visual qualities for the same scene. One can easily spot their differences in motion patterns. Next, we show the 1D histograms based on the velocity field of the videos.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/gt_v_1d-480.webp 480w,/blog/2024/fvmd/gt_v_1d-800.webp 800w,/blog/2024/fvmd/gt_v_1d-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/gt_v_1d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/disco_v_1d-480.webp 480w,/blog/2024/fvmd/disco_v_1d-800.webp 800w,/blog/2024/fvmd/disco_v_1d-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/disco_v_1d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/anyone_v_1d-480.webp 480w,/blog/2024/fvmd/anyone_v_1d-800.webp 800w,/blog/2024/fvmd/anyone_v_1d-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/anyone_v_1d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Dense 1D histograms for the velocity fields of the videos. Left: Ground-truth video. Middle and right: Generated videos for the same scene of worse (middle) and better (right) quality, respectively. </div> <p>The low-quality video has more abrupt motion changes, resulting in a substantially greater number of large-angle velocity vectors. Therefore, the <strong>higher-quality video (right) has a motion pattern closer to the ground-truth video (left) than the lower-quality video (middle)</strong>. This is exactly what we want to observe in the motion features! These features can capture the motion patterns effectively and distinguish between videos of different qualities.</p> <details> <summary>click here for 2D histogram result</summary> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/gt_v_2d-480.webp 480w,/blog/2024/fvmd/gt_v_2d-800.webp 800w,/blog/2024/fvmd/gt_v_2d-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/gt_v_2d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/disco_v_2d-480.webp 480w,/blog/2024/fvmd/disco_v_2d-800.webp 800w,/blog/2024/fvmd/disco_v_2d-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/disco_v_2d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/anyone_v_2d-480.webp 480w,/blog/2024/fvmd/anyone_v_2d-800.webp 800w,/blog/2024/fvmd/anyone_v_2d-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/anyone_v_2d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Dense 2D histograms for the velocity fields of the videos. Left: ground-truth video. Middle and right: Generated videos of worse and better quality, respectively. </div> We can observe similar patterns in the 2D histograms. The higher-quality video (right) has a motion pattern closer to the ground-truth video (left) than the lower-quality video (middle). The unnatural jittering and unsmooth motion in the lower-quality video lead to more frequent large-magnitude velocity vectors, as captured by the 2D histograms. </details> <h3 id="fréchet-video-motion-distance">Fréchet Video Motion Distance</h3> <p>After extracting motion features from video segments of generated and ground-truth video sets, FVMD measures their similarity using the <strong>Fréchet distance</strong> <d-cite key="dowson1982frechet"></d-cite>, which explains the name <em>Fréchet Video Motion Distance (FVMD)</em>. To make the computation tractable, multi-dimensional Gaussian distributions are used to represent the motion features, following previous works <d-cite key="heusel2017gans"></d-cite>. Let $\mu_{\text{gen}}$ and $\mu_{\text{data}}$ be the mean vectors, and $\Sigma_{\text{gen}}$ and $\Sigma_{\text{data}}$ be the covariance matrices of the generated and ground-truth videos, respectively. The FVMD is defined as:</p> \[d_F = ||\mu_{\text{data}}-\mu_{\text{gen}}{||}_2^2 + \mathrm{tr}\left(\Sigma_{\text{data}} + \Sigma_{\text{gen}} -2(\Sigma_{\text{data}}\Sigma_{\text{gen}})^{\frac{1}{2}}\right)\] <h2 id="experiments">Experiments</h2> <p>The ultimate aim of a video evaluation metric is to align with human perception. To validate the effectiveness of FVMD, a series of experiments is conducted in the paper, including <strong>sanity check</strong>, <strong>sensitivity analysis</strong>, and <strong>quantitative comparison</strong> with existing metrics. <strong>Large-scale human studies</strong> are also performed to compare the performance of FVMD with other metrics.</p> <h3 id="sanity-check">Sanity Check</h3> <p>To verify the efficacy of the extracted motion features in representing motion patterns, a sanity check is performed in the FVMD paper. Motion features based on velocity, acceleration, and their combination are used to compare videos from the same dataset and different datasets.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/sanity_check-480.webp 480w,/blog/2024/fvmd/sanity_check-800.webp 800w,/blog/2024/fvmd/sanity_check-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/sanity_check.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> As sample size increases, same-dataset discrepancies (BAIR <d-cite key="ebert2017self"></d-cite> vs BAIR) converge to zero, while cross-dataset discrepancies (TIKTOK <d-cite key="jafarian2022self"></d-cite> vs BAIR) remain large, verifying the soundness of the FVMD metric. </div> <p>When measuring the FVMD of <strong>two subsets from the same dataset</strong>, it <strong>converges to zero as the sample size increases</strong>, confirming that the motion distribution within the same dataset is consistent. Conversely, the FVMD <strong>remains higher for subsets from different datasets</strong>, showing that their motion patterns exhibit a larger gap compared to those within the same dataset.</p> <h3 id="sensitivity-analysis">Sensitivity Analysis</h3> <p>Moreover, a sensitivity analysis is conducted to evaluate if the proposed metric can effectively detect temporal inconsistencies in generated videos, <em>i.e.</em>, being <strong>numerically sensitive to temporal noises</strong>. To this end, artificially-made temporal noises are injected to the TikTok dancing dataset <d-cite key="jafarian2022self"></d-cite> and FVMD scores are computed to assess its sensitivity to data corruption.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/sensitivity_ana-480.webp 480w,/blog/2024/fvmd/sensitivity_ana-800.webp 800w,/blog/2024/fvmd/sensitivity_ana-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/sensitivity_ana.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The FVMD scores in the presence of various temporal noises are presented. </div> <p>Across the four types of temporal noises injected into the dataset <d-footnote> There are four types of temporary noises in the FVMD paper: 1) local swap: swapping a fraction of consecutive frames in the video sequence, 2) global swap: swapping a fraction of frames in the video sequence with randomly chosen frames, 3) interleaving: weaving the sequence of frames corresponding to multiple different videos to obtain new videos, 4) switching: jumping from one video to another video. </d-footnote>, <strong>FVMD based on combined velocity and acceleration features</strong> demonstrates the most reliable performance. It shows a strong negative correlation with noise level, indicating FVMD’s sensitivity to temporal noise and its effectiveness in detecting temporal inconsistencies in generated videos.</p> <h3 id="quantitative-results">Quantitative Results</h3> <p>Further, FVMD provides a quantitative comparison of various video evaluation metrics on TikTok dataset <d-cite key="jafarian2022self"></d-cite>. Fifty videos are generated using different checkpoints named (a) through (e) <d-footnote>The video samples are reproduced from the following models: (a) is from Magic Animate <d-cite key="xu2023magicanimate"></d-cite>; (b), (c), and (e) are from Animate Anyone <d-cite key="hu2023animate"></d-cite>, each with different training hyperparameters; and (d) is from DisCo <d-cite key="wang2023disco"></d-cite>.</d-footnote> and their performance is measured using the FVD <d-cite key="unterthiner2018towards"></d-cite>, FID-VID <d-cite key="heusel2017gans"></d-cite>, VBench <d-cite key="huang2023vbench"></d-cite>, and FVMD metrics. Note that the models (a) to (e) are sorted based on human ratings collected through a user study, from worse to better visual quality (model (e) has the best visual quality and model (a) has the worst). This allows for a comparison of <strong>how well the evaluation metrics align with human judgments</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/blog/2024/fvmd/FVMD.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Video samples created by various video generative models trained on the TikTok dataset <d-cite key="jafarian2022self"></d-cite> are shown to compare the fidelity of different evaluation metrics. </div> <table> <thead> <tr> <th><strong>Metrics</strong></th> <th><strong>Model (a)</strong></th> <th><strong>Model (b)</strong></th> <th><strong>Model (c)</strong></th> <th><strong>Model (d)</strong></th> <th><strong>Model (e)</strong></th> <th><strong>Human Corr.↑</strong></th> </tr> </thead> <tbody> <tr> <td>FID↓</td> <td>73.20 (3rd)</td> <td>79.35 (4th)</td> <td>63.15 (2nd)</td> <td>89.57 (5th)</td> <td>18.94 (1st)</td> <td>0.3</td> </tr> <tr> <td>FVD↓</td> <td>405.26 (4th)</td> <td>468.50 (5th)</td> <td>247.37 (2nd)</td> <td>358.17 (3rd)</td> <td>147.90 (1st)</td> <td>0.8</td> </tr> <tr> <td>VBench↑</td> <td>0.7430 (5th)</td> <td>0.7556 (4th)</td> <td>0.7841 (2nd)</td> <td>0.7711 (3rd)</td> <td>0.8244 (1st)</td> <td>0.9</td> </tr> <tr> <td>FVMD↓</td> <td>7765.91 (5th)</td> <td>3178.80 (4th)</td> <td>2376.00 (3rd)</td> <td>1677.84 (2nd)</td> <td>926.55 (1st)</td> <td><strong>1.0</strong></td> </tr> </tbody> </table> <p>FVMD ranks the models correctly in line with human ratings and has <strong>the highest correlation to human perceptions</strong>. Moreover, FVMD provides <strong>distinct scores for video samples of different quality</strong>, showing a clearer separation between models.</p> <h3 id="human-study">Human Study</h3> <p>In the paper, large-scale human studies are conducted to validate that the proposed FVMD metric aligns with human perceptions. Three different human pose-guided generative models are fine-tuned: DisCo <d-cite key="wang2023disco"></d-cite>, Animate Anyone <d-cite key="xu2023magicanimate"></d-cite>, and Magic Animate <d-cite key="xu2023magicanimate"></d-cite>. These models, with distinct architectures and hyper-parameter settings, yield over 300 checkpoints with varying sample qualities. Users are then asked to compare samples from each pair of models to form a ground-truth user score. All checkpoints are also automatically evaluated using the FVMD metric, and the results are compared with FID-VID <d-cite key="heusel2017gans"></d-cite>, FVD <d-cite key="unterthiner2018towards"></d-cite>, SSIM <d-cite key="wang2004image"></d-cite>, PSNR <d-cite key="wang2004image"></d-cite>, and VBench <d-cite key="huang2023vbench"></d-cite>. <strong>The correlation between the scores given by each metric and the ground-truth user scores is calculated to further assess the performance of each metric.</strong></p> <p>Following the model selection strategy in <d-cite key="unterthiner2018towards"></d-cite>, two settings for the human studies are designed. The first setup is <strong>One Metric Equal</strong>. In this approach, a group of models with nearly identical scores based on a selected metric is identified. Namely, the selected models’ generated samples are considered to have similar visual quality compared to the reference data, according to the selected metric. This setup investigates whether the other metrics and human raters can effectively differentiate between these models.</p> <p>The second setting, <strong>One Metric Diverse</strong>, evaluates the agreement among different metrics when a single metric provides a clear ranking of the performances of the considered video generative models. Specifically, model checkpoints whose samples can be clearly differentiated according to the given metric are selected to test the consistency between this metric, other metrics, and human raters.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/human_study_eql-480.webp 480w,/blog/2024/fvmd/human_study_eql-800.webp 800w,/blog/2024/fvmd/human_study_eql-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/human_study_eql.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 1: Pearson correlation for the One Metric Equal experiments. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/human_study_div-480.webp 480w,/blog/2024/fvmd/human_study_div-800.webp 800w,/blog/2024/fvmd/human_study_div-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/human_study_div.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 2: Pearson correlation for One Metric Diverse experiments. </div> <p>The Pearson correlations range in [-1, 1], with values closer to -1 or 1 indicating stronger negative or positive correlation, respectively. The agreement rate among raters is reported as a percentage from 0 to 1. A higher agreement rate indicates a stronger consensus among human raters and higher confidence in the ground-truth user scores. The correlation is higher-the-better for all metrics in both <strong>One Metric Equal</strong> and <strong>One Metric Diverse</strong> settings. Overall, FVMD demonstrates the strongest capability to distinguish videos when other metrics fall short.</p> <h2 id="summary">Summary</h2> <p>In this blog, we give a brief summary of the recently-proposed <strong>Fréchet Video Motion Distance (FVMD)</strong> metric and its advantages over existing metrics. FVMD is designed to evaluate the motion consistency of generated videos by comparing the discrepancies of velocity and acceleration patterns between generated and ground-truth videos. The metric is validated through a series of experiments, including a sanity check, sensitivity analysis, quantitative comparison, and large-scale human studies. The results show that FVMD outperforms existing metrics in many aspects, such as better alignment with human judgment and a stronger capability to distinguish videos of different quality.</p>]]></content><author><name>Jiahe Liu</name></author><category term="metrics"/><category term="video"/><category term="generative-models"/><summary type="html"><![CDATA[In this blog post, we introduce a promising new metric for video generative models, Fréchet Video Motion Distance (FVMD), which focuses on the motion consistency of generated videos.]]></summary></entry><entry><title type="html">A Review of Video Evaluation Metrics</title><link href="https://dsl-lab.github.io/blog/2024/fvmd-1/" rel="alternate" type="text/html" title="A Review of Video Evaluation Metrics"/><published>2024-06-20T00:00:00+00:00</published><updated>2024-06-20T00:00:00+00:00</updated><id>https://dsl-lab.github.io/blog/2024/fvmd-1</id><content type="html" xml:base="https://dsl-lab.github.io/blog/2024/fvmd-1/"><![CDATA[<h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/video-metrics-480.webp 480w,/blog/2024/fvmd/video-metrics-800.webp 800w,/blog/2024/fvmd/video-metrics-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/video-metrics.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Video evaluation metrics fall into two categories: 1) set-to-set comparison metrics and 2) unary metrics <d-cite key="melnik2024video"></d-cite>. </div> <p>Video generative models have been booming recently with the advent of powerful deep learning architectures and large-scale video datasets. However, evaluating the quality of generated videos remains a challenging task. The lack of a robust and reliable metric makes it difficult to assess the performance of video generative models quantitatively.</p> <p>Arguably, the ultimate goal for video evaluation metric is to <strong>align with human judgment</strong>: the desideratum is for generative models to create videos that meet our aesthetic standards <d-footnote>To demonstrate the quality of a new model, human subjects usually rate its generated samples in comparison to an existing baseline. Subjects are usually presented with pairs of generated clips from two different video models. They are then asked to indicate which of the two examples they prefer in regard to a specific evaluation criterion. Depending on the study, the ratings can either purely reflect the subject's personal preference, or they can refer to specific aspects of the video such as temporal consistency and adherence to the prompt. See <d-cite key="huang2023vbench, liu2024fvmd"></d-cite> for details. </d-footnote>. Humans are very good at judging what <em>looks natural</em> and identifying small temporal inconsistencies. However, the downsides of human ratings, like in every other human-in-the-loop machine learning task, are poor scalability and high costs. For this reason, it is important to develop automated evaluation metrics for model development and related purposes <d-footnote>Human studies can not only be used to evaluate model performance but also to measure how well the automated metrics align with human preferences. Specifically, they can statistically evaluate whether human judgments agree with metric-given results when assessing similar videos <d-cite key="unterthiner2018towards, huang2023vbench, liu2024fvmd"></d-cite>.</d-footnote>.</p> <p>Video evaluation metrics can be categorized into two types: <strong>1) set-to-set comparison metrics</strong> and <strong>2) unary metrics</strong>. The first type measures the difference between the generated set of data and the reference dataset, typically using statistical measures such as the Fréchet distance <d-cite key="dowson1982frechet"></d-cite>. The second type, unary metrics, does not require a reference set, making them suitable for video generation in the wild or video editing, where a gold-standard reference is absent.</p> <p>Below, we elaborate on the most commonly used video evaluation metrics and provide a quantitative comparison of these metrics on the TikTok dataset.</p> <h2 id="set-to-set-comparison-metrics">Set-to-set Comparison Metrics</h2> <p>Set-to-set metrics evaluate the disparity between a generated dataset and a reference dataset, usually within the feature space.</p> <p><strong>Fréchet Inception Distance (FID)</strong> <d-cite key="heusel2017gans"></d-cite> was originally proposed to measure the similarity between the output distribution of an <em>image</em> generative model and its training data. Generated images are first passed through a pre-trained Inception Net <d-cite key="szegedy2016rethinking"></d-cite> to extract features, which are then used to calculate the Fréchet distance between the real and synthetic data distributions. It has been extended to the video domain by computing the FID between the features of <em>individual frames</em> in the generated and reference videos. However, as one could imagine, <strong>this metric does not consider the temporal coherence between frames</strong>.</p> <p><strong>Fréchet Video Distance (FVD)</strong> <d-cite key="unterthiner2018towards"></d-cite> has been proposed as an extension of FID for the video domain. Its backbone is replaced by a 3D ConvNet pre-trained on action recognition tasks in YouTube videos (I3D <d-cite key="carreira2017quo"></d-cite>). The authors acknowledge that the FVD measure is not only <strong>sensitive to spatial degradation</strong> (different kinds of noise) but also to <strong>temporal aberrations</strong> such as the swapping of video frames. <strong>Kernel Video Distance (KVD)</strong> <d-cite key="unterthiner2018towards"></d-cite> is an alternative to FVD proposed in the same work, using a polynomial kernel. It is computed in an analogous manner, except that a polynomial kernel is applied to the features of the Inception Net. However, FVD was found to align better with human judgments than KVD. Nevertheless, both are commonly reported as benchmark metrics for unconditional video generation.</p> <p><strong>Fréchet Video Motion Distance (FVMD)</strong> <d-cite key="liu2024fvmd"></d-cite> is a metric focused on temporal consistency, <strong>measuring the similarity between motion features of generated and reference videos</strong> using Fréchet Distance. It begins by tracking keypoints using the pre-trained PIPs++ model <d-cite key="zheng2023pointodyssey"></d-cite>, then calculates the velocity and acceleration fields for each frame. The metric aggregates these features into statistical histograms and measures their differences using the Fréchet Distance. FVMD assesses motion consistency by analyzing speed and acceleration patterns, assuming smooth motions should follow physical laws and avoid abrupt changes.</p> <p>In addition to these modern video-based metrics, the traditional <strong>Peak Signal-to-Noise Ratio (PSNR)</strong> and <strong>Structural Similarity Index Measure (SSIM)</strong> <d-cite key="wang2004image"></d-cite> are image-level metrics for video quality assessment. Specifically, SSIM characterizes the brightness, contrast, and structural attributes of the reference and generated videos, while PSNR quantifies the ratio of the peak signal to the Mean Squared Error (MSE). Originally proposed for imaging tasks such as super-resolution and in-painting, these metrics are nonetheless repurposed for video evaluation. Unlike the aforementioned methods, PSNR and SSIM do not need pre-trained models. <strong>Nor do they consider the temporal coherence between frames</strong>, which is crucial for video generation tasks.</p> <h2 id="unary-metrics">Unary Metrics</h2> <p>Unary metrics assess the quality of given video samples without the need for a reference set, making them ideal for applications such as video generation in the wild or video editing where a gold-standard reference is unavailable.</p> <p><strong>VBench</strong> <d-cite key="huang2023vbench"></d-cite> proposes a comprehensive set of fine-grained video evaluation metrics to assess <strong>temporal and frame-wise video quality, as well as video-text consistency</strong> in terms of semantics and style. They employ a number of pre-trained models, e.g., RAFT <d-cite key="teed2020raft"></d-cite> for dynamic degree, and MUSIQ <d-cite key="ke2021musiq"></d-cite> for imaging quality, along with heuristics-inspired algorithms, e.g., visual smoothness and temporal flickering, based on inter-frame interpolation and reconstruction error. The overall score is determined by a weighted sum of a number of fine-grained metrics, and the authors also conduct human studies to validate the effectiveness of these metrics.</p> <p>For <strong>text-to-video generation tasks</strong>, <strong>CLIP cosine similarity</strong> is often used to measure the consistency between text prompts and video frames. CLIP <d-cite key="radford2021learning"></d-cite> is a family of vision transformer auto-encoders that map image and text data into a shared embedding space <d-footnote>During training, the distance between embedded images and their associated text labels is minimized through self-supervised learning objective. Thereby, visual concepts are represented close to words that describe them in the embedding space.</d-footnote>. The similarity between text and image CLIP embeddings is measured through cosine distance, where a value of 1 indicates identical concepts, and -1 implies completely unrelated concepts. To determine how well a video sequence adheres to the text prompt, the average similarity between each video frame and the text prompt is calculated (<strong>prompt consistency</strong>) <d-cite key="esser2023structure"></d-cite>. Temporal coherence can be assessed by computing the mean CLIP similarity between adjacent video frames (<strong>frame consistency</strong>). In video editing tasks, the percentage of frames with a higher prompt consistency score than in the original is also reported (<strong>frame accuracy</strong>) <d-cite key="qi2023fatezero"></d-cite>.</p> <p>For generative models trained on <strong>video data with categorical labels</strong>, the <strong>Inception Score (IS)</strong> <d-cite key="salimans2016improved"></d-cite> is a widely used metric. Similar to FID, IS was originally proposed for image generation tasks: an Inception Net <d-cite key="szegedy2016rethinking"></d-cite> classifier pre-trained on the ImageNet dataset <d-cite key="deng2009imagenet"></d-cite> is first used to predict the class labels of each generated image. The IS score is then calculated using the Kullback-Leibler divergence between the conditional class probability distribution $p(y|x)$ and the marginal class distribution $p(y)$ of the generated samples, where $y$ is the discrete label and $x$ is the generated image. It has been generalized to the video domain <d-cite key="saito2020train"></d-cite>, specifically for the UCF101 dataset <d-cite key="soomro2012ucf101"></d-cite>, where a pre-trained action recognition classifier (C3D <d-cite key="tran2015learning"></d-cite>) is used for score computation. However, this metric in practice is <strong>highly specific to the UCF101 dataset</strong> and is hardly applicable to videos in the wild due to classification difficulty.</p> <h2 id="comparison-on-tiktok-dataset">Comparison on TikTok Dataset</h2> <p>Let’s see how these evaluation metrics work in real life! We adopt a generic setup without using text prompts or discrete labels in the video generation task. We use the TikTok dataset <d-cite key="jafarian2022self"></d-cite> to provide a quantitative comparison of various video evaluation metrics.</p> <p>Specifically, we generate 50 videos using different checkpoints named (a) through (e) <d-footnote>The video samples are reproduced from the following models: (a) is from Magic Animate <d-cite key="xu2023magicanimate"></d-cite>; (b), (c), and (e) are from Animate Anyone <d-cite key="hu2023animate"></d-cite>, each with different training hyperparameters; and (d) is from DisCo <d-cite key="wang2023disco"></d-cite>.</d-footnote> and measure their performance using the FVD <d-cite key="unterthiner2018towards"></d-cite>, FID <d-cite key="heusel2017gans"></d-cite>, VBench <d-cite key="huang2023vbench"></d-cite>, and FVMD <d-cite key="liu2024fvmd"></d-cite> metrics. We do not use CLIP or IS in this comparison, as they are not suitable for our setup. The models (a) to (e) are sorted based on human ratings collected through a user study, from worse to better visual quality <d-cite key="liu2024fvmd"></d-cite> (model (e) has the best visual quality and model (a) has the worst). We can then <strong>compare how well the evaluation metrics align with human judgments</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/blog/2024/fvmd/FVMD.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls="" loop=""/> </figure> </div> </div> <div class="caption"> We evaluate video samples created by various video generative models trained on the TikTok dataset <d-cite key="jafarian2022self"></d-cite> to compare the fidelity of different evaluation metrics. </div> <p>We put together a couple of videos generated by different models which clearly differ in visual quality. Models (a), (b), and (c) result in videos with incomplete human shapes and unnatural motions. Model (d) produces a video with better visual quality, but the motion is still not smooth, resulting in a lot of flickering. In comparison, model (e) generates a video with better visual quality and motion consistency. <em>Disclaimer: These video samples are nowhere near perfect; however, they are sufficient to compare different evaluation metrics.</em></p> <p><strong>Quantitative Results.</strong></p> <table> <thead> <tr> <th><strong>Metrics</strong></th> <th><strong>Model (a)</strong></th> <th><strong>Model (b)</strong></th> <th><strong>Model (c)</strong></th> <th><strong>Model (d)</strong></th> <th><strong>Model (e)</strong></th> <th><strong>Human Corr.↑</strong></th> </tr> </thead> <tbody> <tr> <td>FID↓</td> <td>73.20 (3rd)</td> <td>79.35 (4th)</td> <td>63.15 (2nd)</td> <td>89.57 (5th)</td> <td>18.94 (1st)</td> <td>0.3</td> </tr> <tr> <td>FVD↓</td> <td>405.26 (4th)</td> <td>468.50 (5th)</td> <td>247.37 (2nd)</td> <td>358.17 (3rd)</td> <td>147.90 (1st)</td> <td>0.8</td> </tr> <tr> <td>VBench↑</td> <td>0.7430 (5th)</td> <td>0.7556 (4th)</td> <td>0.7841 (2nd)</td> <td>0.7711 (3rd)</td> <td>0.8244 (1st)</td> <td>0.9</td> </tr> <tr> <td>FVMD↓</td> <td>7765.91 (5th)</td> <td>3178.80 (4th)</td> <td>2376.00 (3rd)</td> <td>1677.84 (2nd)</td> <td>926.55 (1st)</td> <td><strong>1.0</strong></td> </tr> </tbody> </table> <p>In this table, we show the raw scores given by the metrics, where FVD, FID, and FVMD are lower-is-better metrics, while VBench is higher-is-better. The scores are computed by comparing a set of generated videos (as shown in the video above) to a set of reference videos. We also report the corresponding ranking among the five models based on quantitative results. The ranking correlation between the metrics evaluation and human ratings is also reported, where a higher value indicates better alignment with human judgments.</p> <p>We can see the ambiguity of some evaluation metrics. <strong>Model (a), which has the poorest quality, cannot be effectively distinguished from models (b-d) based on the FID or VBench metrics</strong>. <strong>Additionally, model (c) is mistakenly ranked higher than model (d) by all metrics except for the FVMD metric</strong>. In particular, VBench gives very close scores to models (a-d) with clearly different visual quality, which are not consistent with human judgments. <strong>FVMD, on the other hand, ranks the models correctly in line with human ratings</strong>. Moreover, FVMD gives distinct scores for video samples of different quality, showing a clearer separation between models. This suggests that FVMD is a promising metric for evaluating video generative models, especially when motion consistency is concerned.</p> <p><strong>Frames Comparison.</strong> <br/> We also present visualizations of video frames for one randomly selected scene to further compare the metrics fidelity.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/fig-eval-metric-comparison-v0-480.webp 480w,/blog/2024/fvmd/fig-eval-metric-comparison-v0-800.webp 800w,/blog/2024/fvmd/fig-eval-metric-comparison-v0-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/fig-eval-metric-comparison-v0.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <details> <summary>click here for more frames comparison</summary> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2024/fvmd/fig-eval-metric-comparison-v1-480.webp 480w,/blog/2024/fvmd/fig-eval-metric-comparison-v1-800.webp 800w,/blog/2024/fvmd/fig-eval-metric-comparison-v1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/blog/2024/fvmd/fig-eval-metric-comparison-v1.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> </details> <h2 id="summary">Summary</h2> <p>We review the video evaluation metrics used to assess video generative models. These metrics can be categorized into two types: set-to-set comparison metrics (FID, FVD, KVD, FVMD, PSNR, and SSIM) and unary metrics (VBench, CLIP score, and IS). We discuss the pros and cons of each type and provide a detailed comparison using the TikTok dataset. The results show that the <strong>FVMD metric aligns better with human judgments than other metrics, especially for assessing motion consistency</strong>. This suggests that FVMD is a promising metric for evaluating video generative models.</p> <p>Wonder why FVMD performs so much better than other metrics? Check out <a href="https://dsl-lab.github.io/blog/2024/fvmd-2/">the second part of our blog post</a> to find out more! We will delve into the details of the FVMD metric and explain why it is more effective in assessing video quality and motion consistency.</p>]]></content><author><name>Qi Yan</name></author><category term="metrics"/><category term="video"/><category term="generative-models"/><summary type="html"><![CDATA[Video generative models have been rapidly improving recently, but how do we evaluate them efficiently and effectively? In this blog post, we review the existing evaluation metrics and highlight their pros and cons.]]></summary></entry><entry><title type="html">Flows and Flow Matching</title><link href="https://dsl-lab.github.io/blog/2024/flows/" rel="alternate" type="text/html" title="Flows and Flow Matching"/><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://dsl-lab.github.io/blog/2024/flows</id><content type="html" xml:base="https://dsl-lab.github.io/blog/2024/flows/"><![CDATA[<h1 id="flows-and-flow-matching">Flows and Flow Matching</h1> <p>This blog post will provide a brief introduction to flow-based models and flow matching.</p> <p>Flow-based models are an example of a probabilistic generative model. The goal of probablisitic modeling is to model the distribution of a random variable \(X\). This is typically done in a supervised fashion using examples \(\{x^{(i)}\}_{i=1}^N\) collected from the data distribution. We learn to approximate the probability density function of the data distribution with a model \(p(x;\theta)\) where \(\theta\) represents the parameters of a neural network. Specifically, flow-based models learn to model the data distribution through normalizing flows. The main benefit of flow-based models is that they are effecient for sample generation and likelihood evaluation.</p> <h2 id="normalizing-flows">Normalizing Flows</h2> <p>The framework for normalizing flows is based on a rather simple fact from probability theory. Suppose \(\mathbf{x_0} \in \mathbb{R}^d\) is distributed according to \(p\) i.e. \(\mathbf{x_0} \sim p\). Let \(f: \mathbb{R}^d \to \mathbb{R}^d\) be an invertible and differentiable function. Now, let’s do a change of variables, \(\mathbf{x_1} = f(\mathbf{x_0})\). We can write \(q\), the distribution of the transformed variable \(\mathbf{x_1}\) in terms of \(p\). Namely,</p> <p>\(\begin{align} q(\mathbf{x_1}) &amp;= p(\mathbf{x_0})\left|\det \frac{\partial f^{-1}}{\partial \mathbf{x_1}}(\mathbf{x_1})\right| \notag \\ &amp;= p\left(f^{-1}(\mathbf{x_1})\right)\left|\det \frac{\partial f^{-1}}{\partial \mathbf{x_1}}(\mathbf{x_1})\right|. \end{align}\) The notation \(\frac{\partial f^{-1}}{\partial \mathbf{x_1}}\) denotes the Jacobian of \(f^{-1}\). Also, because the transformation is invertible, we can obtain \(p\) from \(q\) too:</p> \[\begin{align*} p(\mathbf{x_0}) &amp;= q(\mathbf{x_1})\left|\det \frac{\partial f}{\partial \mathbf{x_0}}(\mathbf{x_0}) \right| \\ &amp;= q(f(\mathbf{x_0}))\left|\det \frac{\partial f}{\partial \mathbf{x_0}}(\mathbf{x_0}) \right|. \end{align*}\] <p><b> Example 1 </b>. Scaling and shifting a Gaussian. Suppose \(\mathbf{x_0} \in \mathbb{R}\) and \(\mathbf{x_0} \sim \mathcal{N}(0,1)\). Let \(\mathbf{x_1} = f(\mathbf{x_0}) = \sigma \mathbf{x_0} + \mathbf{\mu}\). Then \(\mathbf{x_0} = f^{-1}(\mathbf{x_1}) = \frac{\mathbf{x_1} - \mathbf{\mu}}{\sigma}\) so \(\frac{df^{-1}}{d\mathbf{x_1}} = \frac{1}{\sigma}\). In this case, the Jacobian is a positive scalar function so the determinant is itself. Recall the pdf of a canonical Gaussian:</p> \[p(\mathbf{x_0}) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\mathbf{x_0}^2}.\] <p>Applying the formula we obtain a Gaussian with mean \(\mu\) and variance \(\sigma^2\),</p> \[\begin{align*} q(\mathbf{x_1}) &amp;= p\left(f^{-1}(\mathbf{x_1})\right)\left|\det \frac{\partial f^{-1}}{\partial \mathbf{x_1}}(\mathbf{x_1})\right| \\ &amp;= \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x - \mathbf{\mu}}{\sigma})^2}\frac{1}{\sigma} \\ &amp;= \frac{1}{\sqrt{2\pi\sigma}}e^\frac{-(x-\mathbf{\mu})^2}{2\sigma^2}. \end{align*}\] <p>Intuitively, multiplying \(\mathbf{x_0}\) by \(\sigma\) stretches the domain which changes the variance of the Gaussian. Adding \(\mu\) applies a shift to this stretched Gaussian.</p> <p><b> Example 2 </b>. Non-linear transformation of Gaussian. Suppose \(\begin{bmatrix} x \\ y\end{bmatrix} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\). The pdf of a canonical Gaussian in 2D is:</p> \[p(x,y) = \frac{1}{\sqrt{2\pi}}e^\frac{-(x^2 + y^2)}{2}.\] <p>Let’s apply a cubic transformation to each coordinate, \(u = x^3\) and \(v = y^3\). The inverse is \(x = u^\frac{1}{3}\) and \(y = v^\frac{1}{3}\). The Jacobian of this transformation is the following:</p> \[\begin{bmatrix} \frac{\partial x}{\partial u} &amp; \frac{\partial v}{\partial v} \\ \frac{\partial y}{\partial u} &amp; \frac{\partial v}{\partial v} \\ \end{bmatrix} = \begin{bmatrix} \frac{1}{3}u^{-\frac{2}{3}} &amp; 0 \\ 0 &amp; \frac{1}{3}v^{-\frac{2}{3}}\\ \end{bmatrix}.\] <p>The absolute value of the determinant of this matrix is \(\frac{1}{9}\lvert uv\rvert ^{-\frac{2}{3}}\). Therefore,</p> \[\begin{align*} q(u, v) &amp;= \frac{1}{9}\lvert uv\rvert ^{-\frac{2}{3}} p(x,y) \\ &amp;= \frac{1}{9}\lvert uv\rvert ^{-\frac{2}{3}}p(u^\frac{1}{3}, v^\frac{1}{3}) \\ &amp;= \frac{\lvert uv\rvert ^{-\frac{2}{3}}}{9\sqrt{2\pi}}e^\frac{-(u^\frac{2}{3} + v^\frac{2}{3})}{2} \\ \end{align*}\] <div class="container"> <div class="row justify-content-md-center"> <div class="col-md-6"> <img class="img-responsive" src="ex2_1.png"/> </div> <div class="col-md-6"> <img class="img-responsive" src="ex2_2.png"/> </div> </div> <div class="caption"> By applying a cubic transformation to a Gaussian, we obtained a slightly more complex distribution. </div> </div> <p>In the context of normalizing flows, \(p\) is simple distribution, typically a canonical Gaussian. Our goal with this setup is to learn how to transform samples from \(p\) to samples from a complex data distribution \(q\). We can do this by learning the invertible transformation \(f\). The function \(f\) will involve the use a neural network with parameters \(\theta\), so from now on we will denote the transformation as \(f_\theta\). Once we have learned \(f_\theta\) we will have access to \(\hat{q}\) which hopefully will be a good approximation of \(q\).</p> <p>Given that we learned \(f_\theta\), how do we do density estimation and generate samples from \(q\)? This is quite simple for flow models. If you have a data sample \(\mathbf{x}^{(i)}\), you can compute \(f^{-1}(\mathbf{x}^{(i)})\) and the deterimant of the Jacobian. Then plug those into eq. (1) to obtain \(\hat{q}(\mathbf{x}^{(i)})\). If you want to sample from \(q\), first obtain a sample \(\mathbf{x_0} \sim p\) which we know how to do because \(p\) is a simple distribution. Then, we can compute \({\mathbf{x_1} = f^{-1}_\theta(\mathbf{x_0})}\) and so \(\mathbf{x_1}\) will be a sample from \(\hat{q}\).</p> <p>Normalizing flow methods impose a specific structure on \(f_\theta\). We want to learn the transformation from \(p\) to \(q\) as a sequence of simpler transformations. Define functions \(f_1 \cdots f_k\) to be invertible and differentiable. Note these functions are still parameterized by \(\theta\) but we omit making this explicit for sake of notation. Invertible and differentiable functions are closed under composition. We can use this fact to define \(f_\theta\) in the following manner:</p> \[f_\theta = f_k \circ f_{k-1} \cdots f_2 \circ f_1.\] <p>The intituion behind this formulation is somewhat analagous to the justification of stacking many layers in a deep learning model instead of using one wide layer. Learning the transformation from \(p\) to \(q\) in one step might be too difficult. Instead, we can learn a sequence of functions where each function is responsible for transforming its input distribution into a slightly more complex distribution. Eventually, we are able to model the complexity of the data distribution.</p> <p><img src="norm_flow.png" alt="alt text"/> <em>Figure 1: Each function transforms an input distrubtion into a slightly more complex distribution. The overall transformation maps the simple distribution to the complex data distribution.</em></p> <p>Let’s reformulate the process of normalizing flows. Since we are performing multiple steps, \(\mathbf{x_1}\) is no longer a sample from \(q\) but a sample from a distribution slightly more complex than \(p_0 = p\). After applying \(K\) transformations we will have that \(\mathbf{x_K} \sim \hat{q}\):</p> \[\begin{align*} &amp;\phantom{\Rightarrow} \ \ \mathbf{x_0} \sim p_0, \quad \mathbf{x_1} = f_1(\mathbf{x_0}) \\ &amp;\Rightarrow \mathbf{x_1} \sim p_1, \quad \mathbf{x_2} = f_2(\mathbf{x_1}) \\ \phantom{\Rightarrow x_1} &amp;\cdots \\ &amp;\Rightarrow \mathbf{x}_{K-1} \sim p_{K-1}, \quad \mathbf{x}_K = f_K(\mathbf{x}_{K-1}) \\ &amp;\Rightarrow \mathbf{x}_K \sim p_K = \hat{q} \approx q. \end{align*}\] <p>The sequence of transformations from \(p\) to the distribution \(q\) is called a flow. The term normalizing in normalizing flow refers to the fact that after a transformation is applied, the resulting pdf is valid i.e. it integrates to one over its support and is greater than zero.</p> <p>So how do we actually train normalizing flows? The objective function is simply the maximum log-likelihood of the data:</p> \[\begin{align*} \theta^* &amp;= \max_{\theta} \sum_{i=1}^{N} \log(\hat{q}(\mathbf{x}^{(i)})) \\ &amp;= \max_{\theta} \sum_{i=1}^{N} \log\left(p\left(f^{-1}_\theta(\mathbf{x}^{(i)})\right)\left|\det \frac{\partial f^{-1}_\theta}{\partial \mathbf{x}_K}(\mathbf{x}^{(i)})\right|\right) \\ &amp;= \max_{\theta} \sum_{i=1}^{N} \log p\left(f^{-1}_\theta(\mathbf{x}^{(i)})\right) + \log\left|\det \frac{\partial f^{-1}_\theta}{\partial \mathbf{x}_K}(\mathbf{x}^{(i)})\right| \end{align*}.\] <p>Remember that \(f_\theta\) is actually the composition of a sequence of functions. We can simplify the determinant of the Jacobian of \(f\) by decomposing it as a product of the individual determinants. Specifically,</p> \[\left| \det \frac{f^{-1}_\theta}{\partial \mathbf{x}_K} \right| = \left| \det \prod_{k=1}^K \frac{f^{-1}_k}{\partial \mathbf{x}_k} \right| = \prod_{k=1}^K \left| \det \frac{f^{-1}_k}{\partial \mathbf{x}_k} \right|.\] <p>Substituting this back into the objective function we obtain:</p> \[\max_{\theta} \sum_{i=1}^{N} \left[ \log p\left(f^{-1}_\theta(\mathbf{x}^{(i)})\right) + \sum_{k=1}^{K} \log\left|\det \frac{f^{-1}_k}{\partial \mathbf{x}_k} (\mathbf{x}^{(i)}) \right|\right]\] <p>We can intepret the sum of log determinants in the objective as each “layer” of the flow receiving additional gradient information about the objective.</p> <p>Research in normalizing flow methods typically consists of constructing transformations that are easily invertible and have simple and computable log determinants. There are many normalizing flow methods such as RealNVP, NICE, Glow etc. Here we will cover residual flows which will provide a good context for continuous normalizing flows.</p> <h2 id="continuous-normalizing-flows">Continuous Normalizing Flows</h2> <p>In the normalizing flows setup, the transformation from the simple distribution to the data distribution is expressed as a finite composition of functions. We can intepret this as a discrete time process with \(K\) time steps. Each time step, there is a corresponding intermediary distribution. But how can we obtain a transformation from \(p\) to \(q\) in continuous time rather than discrete time? Imagine this as taking an infinite composition of functions. We can express this idea using Ordinary Differential Equations (ODE), the fundamental component of Continuous Normalizing Flows (CNF).</p> <p>To gain some intuition for flows and ODEs, consider a two dimensional vector field \(v(x,y)\) that describes the movement of water along a river. For simplicity, assume it’s time-independent. The velocity of the water at point \((x,y)\) is the vector \(v(x,y)\). The path of a pebble thrown into the water at time \(t=0\) is a curve we can parameterize as a function of time:</p> \[\mathbf{r}(t) = \langle x(t), y(t) \rangle, \qquad \mathbf{r}(0) = \langle x(0), y(0) \rangle.\] <p>We can solve for the position of the pebble at time \(t\) by making the following observation. At time \(t\), the velocity of the pebble, \(\frac{d\mathbf{r}(t)}{dt}\), is the same as the velocity of the water at the position of the pebble, \(\mathbf{r}(t)\). We can model this with the following ODE:</p> \[\frac{d\mathbf{r}(t)}{dt} = v(\mathbf{r}(t)) = v(x(t), y(t)), \qquad \mathbf{r}(0) = \langle x(0), y(0) \rangle.\] <p>This example demonstrate how we can describe the movement of a particle induced by a vector field given some initial position. Specifically, we can construct a function \(\mathbf{r}(t)\) that describes the path taken by a single particle starting at a specific point in space at \(t=0\). As we will see, a flow in the context of CNFs is a more general object that represents the motion of all particles through time.</p> <h4 id="example">Example</h4> <p>Put example here with vector field.</p> <p>Let’s provide a more rigorous definition of a flow. Suppose we have a vector field \(u: \mathbb{R}^d \times [0, 1] \to \mathbb{R}^d\). Unlike the example above, this is a time-dependent vector field and we will denote the time parameter as a subscript, \(u_t(x)\). In this setup, \(d\) is the dimension of our data space.</p> <p>A flow, which is induced by the vector field \(v\), is a mapping \(\phi: \mathbb{R}^d \times [0,1] \to \mathbb{R}^d\) which satisfies the following ODE:</p> \[\frac{d\phi_t(x)}{dt} = u_t(\phi_t(x)),\] <p>with initial condition \(\phi_0(x) = x\).</p> <p>To gain a better intiution of what \(\phi\) represents we can compare it to \(\mathbf{r}(t)\). Given some initial point \(\mathbf{x_0}\), \(\mathbf{r}(t)\) is the position of that point at time \(t\) induced by the movement of water. Similarly, when we provide \(\mathbf{x_0}\) as input to \(\phi\), we will get the function \(\phi(t, \mathbf{x_0}): [0, 1] \to \mathbb{R}^d\) which is only a function of time. It parameterizes a curve in \(\mathbb{R}^d\) that represents the position of the point \(\mathbf{x_0}\) with time induced by the vector field \(u_t\). We can view \(\phi\) from another perspective. Given a specific point in time \(t_0 \in [0,1]\) as input to \(\phi\), we will obtain a function \(\phi(t_0, \mathbf{x}): \mathbb{R}^d \to \mathbb{R}^d\). This function maps all points at time \(t=0\) to the position they would be at time \(t=t_0\). Overall, the mapping \(\phi\) describes the movement of all points starting from time \(t=0\) to time \(t = 1\).For consistent notation, we will denote the time parameter as a subscript \(\phi_t\).</p> <p>Another important object in CNFs is the probability density path \({p_t: \mathbb{R}^d \times [0,1] \to \mathbb{R}_{&gt;0}}\). It is a time-dependent probability density function i.e. \(\int p_t(\mathbf{x})d\mathbf{x} = 1\). Similar to normalizing flows, we let \(p_0 = p\) be a simple distribution such as a canonical Gaussian. Then \(p_t\) is defined by a change of variables from \(p_0\) using mapping \(\phi_t\):</p> \[\begin{equation} p_t(\mathbf{x}) = p_0(\phi_t^{-1}(\mathbf{x}))\det \left| \frac{\partial \phi_t^{-1}}{\partial \mathbf{x}}(\mathbf{x}) \right|. \end{equation}\] <p>Note: with some regularity conditions on \(u_t\) we can gaurauntee that \(\phi_t\) is invertible. The details is out of scope for this blog post.</p> <p>In the setting of CNFs, we let \(p_1\) be the data distibution. The goal is to learn a vector field \(v_t\) which induces a flow \(\phi_t\). This flow is responsible for transforming the simple distribution \(p_0 = p\) at time \(t=0\) to the data distribution \(p_1 = q\) at time \(t=1\).</p> <p>The training objective is the same as in normalizing flows. We maximize the log-likelihood of the data. Given a data point \(\mathbf{x_1} \in \mathbb{R}^d\), to compute \(\log p_1(\mathbf{x_1})\) we could use eq (2). However, as in normalizing flows, that would require computing the Jacobian which is an \(O(d^3)\) operation. A benefit of CNFs is that once we are in the continuous setting, there is an alternative method available so we don’t have to do this computation. The alternative method involves the continuity equation:</p> \[\frac{d}{dt}p_t(\mathbf{x}) + \nabla \cdot (p_t(\mathbf{x})u_t(\mathbf{x})) = 0.\] <p>The continuity equation is a Partial Differential Equation (PDE) where \(\nabla \cdot\) represents the divergence operator. The divergence is computed with respect to the spatial dimensions \(\frac{\partial}{\partial x_i}\). The continuity equation provides a necassary and sufficient condition to ensure that a vector field \(u_t\) generates the probability density path \(p_t\). The continiuty equation can be derived using some basic vector calculus and it has a clean physical intepretation. Suppose you have some arbitrary volume \(V\) in \(\mathbb{R}^3\). The main observation is that probability density, \(p_t\) has to integrate to \(1\) over \(\mathbb{R}^3\) by definition. So analagous to mass, it is a conserved quantity. It cannot dissapear or appear out of thin air. Therefore, the change in probability density in the volume must equal the difference in probablity density that has entered the volume and exited the volume. This is the same ideas as if \(u_t\) represented the flow of water and \(p_t\) is the mass of the water. The change in mass must be the difference in the mass of water entering and leaving the volume. The change in probability density in the volume can be written as follows:</p> \[\frac{d}{dt}\iiint_V p_t dV.\] <p>Let \(S\) be the surface (boundary) of the volume. Let \(n: \mathbb{R}^3 \to \mathbb{R}^3\) represent the normal vector to the surface at point \((x,y,z)\). Then the probability density entering and leaving the volume is:</p> \[- \iint_S p_t (u_t \cdot n) dS = - \iiint_V \nabla \cdot (p_tu_t) dV.\] <p>The equality is an application of Gauss’s divergence theorem. Therefore,</p> \[\frac{d}{dt}\iiint_V p_t dV = - \iiint_V \nabla \cdot (p_tu_t) dV\] <p>Moving everything to one side and simplfying we get,</p> \[\iiint_V \frac{d}{dt}p_t + \nabla \cdot (p_tu_t)dV = 0.\] <p>Since this is true for every volume \(V\) it must be that the quantity inside the integral is equal to \(0\) and we arrive at the continuity equation. This reasoning can be applied in arbitrary dimension \(\mathbb{R}^d\).</p> <p>Using the continuity equation and the ODE describing the flow \(\phi_t\) we get the instantaneous change of variable equation:</p> \[\frac{d}{dt}\log p_t(\phi_t(\mathbf{x})) + \nabla \cdot u_t(\phi_t(\mathbf{x})) = 0.\] <p>Now we have an ODE that describes the change of the log-probability along the flow trajectory. So we can use an ODE solver to obtain a solution to this ODE which will give use \(\log p_1(\mathbf{x_1})\). However, the downside to this approach is that we have to simulate the flow trajectory and compute a divergence which may still be expensive to compute. This results in continuous normalizing flows not being a very scalable method. Flow matching aims to solve this issue.</p> <h2 id="flow-matching">Flow Matching</h2> <p>Flow matching builds on the same framework as CNFs but uses a different loss function. Mainly because we would like to train with a more scalable loss function.</p> <p>Notice that by the continuity equation, there is a direct correspondence between the probability density path \(p_t\) and the vector field \(u_t\). Namely, if we knew the vector field already then we know it would generate a unique probability density path. Therefore, instead of directly optimizing the probability density path and computing \(\log p_1(\mathbf{x_1})\) we can optimize the vector field instead. So the flow matching loss looks like this:</p> \[\mathcal{L}_{FM}(\theta) = \mathbb{E}_{t,p_t(\mathbf{x})}\left\lVert v_t(\mathbf{x}) - u_t(\mathbf{x})\right\rVert^2,\] <p>where \(v_t(x)\) is a learnable vector field parameterized by \(\theta\). We let \(p_t\) describe the probability path such that \(p_0 = p\) is a simple distribution e.g. canonical Gaussian and \(p_1\) is approximately the data distribution \(q\). We regress the learnable vector field \(v_t\) onto the true vector field, \(u_t\), that generates the probability density path \(p_t\). However, in practice, we cannot compute this loss because we don’t know \(p_t\) or \(u_t\). If we did then obviously there would be no point in learning the vector field \(v_t\). To overcome this obstacle, we are going to create another loss that will be computable. This is the conditional flow matching loss:</p> \[\mathcal{L}_{CFM}(\theta) = \mathbb{E}_{t,q(\mathbf{x_1}), p_t(\mathbf{x}\vert\mathbf{x_1})}\left\lVert v_t(x) - u_t(\mathbf{x}\vert\mathbf{x_1})\right\rVert^2.\] <p>We can prove that \(\nabla_\theta \mathcal{L}_{FM}\) and \(\nabla_\theta \mathcal{L}_{CFM}\) are equal upto a constant. So they are equivalent in a sense since they have the same optima. Thus the conditional flow matching loss is a reasonable replacement. Now, we will describe all of the objects in the conditional flow matching loss.</p> <p>The basic idea is that we can construct the marginal probability path using conditional probability paths. These conditional paths are conditioned on data samples. Suppose we have a particular sample \(\mathbf{x_1}\) from the data distribution. We design the conditional probability path, \(p_t(x \vert \mathbf{x_1})\) so that \(p_0(\mathbf{x}\vert\mathbf{x_1}) = p\) and \(p_1(\mathbf{x} \vert \mathbf{x_1}) = q\). To satisfy the boundary conditions, we can set \(p_0(\mathbf{x}\vert\mathbf{x_1}) = p(x)\) and \(p_1(\mathbf{x} \vert \mathbf{x_1}) = \delta_{\mathbf{x_1}}\) where \(\delta_{\mathbf{x_1}}\) is the Dirac distribution centered at \(\mathbf{x_1}\). To obtain the marginal probability path, we can marginalize over the data distribution:</p> \[p_t(x) = \int p_t(x|\mathbf{x_1})q(\mathbf{x_1})d\mathbf{x_1}.\] <p>We can see that setting \(p_0(\mathbf{x} \vert\mathbf{x_1}) = p_0(x)\) and \(p_1(\mathbf{x} \vert\mathbf{x_1}) = \delta_{\mathbf{x_1}}\) results in \(p_0(\mathbf{x}) = q(\mathbf{x})\) and \(p_1(\mathbf{x}) = q(\mathbf{x})\). For numerical reasons, instead of using the actual Diract distibution, we can approximate it by setting \(p_1(x\vert\mathbf{x_1}) = \mathcal{N}(\mathbf{x_1}, \sigma^2_{min}\mathbf{I})\) with \(\sigma_{min}\) sufficiently small.</p> <p>For conditional probability path, there exists a conditional vector field which we denote as \(u_t(x\vert\mathbf{x_1})\) which generates it. We can express the marginal vector field as</p> \[\begin{align*} u_t(x) = \int u_t(x \vert \mathbf{x_1})\frac{p_t(x \vert \mathbf{x_1})q(\mathbf{x_1})}{p_t(x)}d\mathbf{x_1}. \end{align*}\] <p>To prove that the marginal vector field \(u_t\) generates the probability path \(p_t\) we show that they satisfy the continuity equation. The conditional flow, \(\phi_t(\mathbf{x}\vert \mathbf{x_1})\) satisfies the following ODE based on the conditional vector field:</p> \[\begin{equation} \frac{d}{dt}\phi_t(\mathbf{x} \vert \mathbf{x_1}) = u_t\left(\phi_t(\mathbf{x} \vert \mathbf{x_1}) \vert \mathbf{x_1}\right), \end{equation}\] <p>with initial condition \(\phi_0(\mathbf{x} \vert \mathbf{x_1}) = \mathbf{x}\). Therefore, we can integrate over the conditional vector field to obtain the conditional flow. Although, the flow matching loss requires computation of \(u_t\). Even if we define \(u_t\) based on conditional vector fields, computing the marginalization integral is still intractable. So we do need to use the conditional flow matching loss.</p> <p>Returning back to the conditional flow matching loss, the main idea is that we take an expectation over the data distribution and the conditional probability path. As a result, we can replace the marginal vector field in the flow matching loss with the conditional vector field. In practice, we sample a point from the dataset and then sample from the conditional probability path instead of the marginal probability path. Of course, computing the loss also involves sampling a time \(t \in [0,1]\).</p> <p>Furthermore, we can make the following observation to reparameterize the conditional flow matching loss. The reparameterization avoids having to sample \(\mathbf{x} \sim p_t(\mathbf{x} \vert \mathbf{x_1})\). Instead, we can sample \(\mathbf{x_0} \sim p\) from the simple distribution. Then \(\mathbf{x_t} = \phi_t(\mathbf{x_0} \vert \mathbf{x_1})\) is a sample from \(p_t(\mathbf{x} \vert \mathbf{x_1})\) since the conditional flow is a transformation from \(p\) to \(p_t(\mathbf{x} \vert \mathbf{x_1})\). Therefore, \(\mathbf{x_t}\) is the solution to the ODE in equation (3) with \(\mathbf{x_0}\) substituted into the flow:</p> \[\frac{d\phi_t(\mathbf{x_0}|\mathbf{x_1})}{dt} = \mu_t(\phi_t(\mathbf{x_0}|\mathbf{x_1}) \vert \mathbf{x_1}),\] <p>with initial condition \(\phi_0(\mathbf{x_0}\vert\mathbf{x_1}) = \mathbf{x_0}\). Therefore, we can rewrite the conditional flow matching objective as:</p> \[\begin{align} \mathcal{L}_{CFM}(\theta) &amp;= \mathbb{E}_{t, q(\mathbf{x_1}), p(\mathbf{x_0})}\left\lVert v_t(\phi_t(\mathbf{x_0}|\mathbf{x_1})) - \mu_t(\phi_t(\mathbf{x_0}|\mathbf{x_1}) | \mathbf{x_1})\right\rVert^2 \notag \\ &amp;= \mathbb{E}_{t, q(\mathbf{x_1}), p(\mathbf{x_0})}\left\lVert v_t(\phi_t(\mathbf{x_0}|\mathbf{x_1})) - \frac{d\phi_t(\mathbf{x_0}|\mathbf{x_1})}{dt}\right\rVert^2. \end{align}\] <p>To summarize, we have a way of training CNFs by using conditional probability paths and flows. The conditional flow matching loss has the same optima and doesn’t require access to the marginal probability path or vector field. We can compute the conditional flow matching loss effeciently as long as \(p_t(x\vert\mathbf{x_1})\) is defined and can be sampled from effeciently. Furthermore, we are able to easily compute \(u_t(x\vert\mathbf{x_1})\) because it is defined on a per-sample basis.</p> <p>Now we have covered the conditional flow matching framework, we have to choose how to define \(p_t(\mathbf{x} \vert \mathbf{x_1})\) and \(\phi_t(\mathbf{x} \vert \mathbf{x_1})\). The definitions for these objects are motivated primarily by simplicity and tractability. Flow matching was introduced as a vector field \(u_t\) inducing a flow \(\phi_t\) that results in a probability density path \(p_t\). Although this is the natural way to understand the framework, we are going to define these objects in the opposite order but everything still works out.</p> <p>We start off by defining the conditional probability path. A natural and simple choice for this is a Gaussian distribution,</p> \[p_t(\mathbf{x}\vert\mathbf{x_1}) = \mathcal{N}(u_t(\mathbf{x_1}), \sigma^2_t(\mathbf{x_1})\mathbf{I}),\] <p>where \(u_t: \mathbb{R}^d \times [0,1] \to \mathbb{R}^d\) and \(\sigma: \mathbb{R}^d \times [0,1] \to \mathbb{R}_{&gt;0}\) are the time-dependent mean and standard deviation of the Gaussian. We choose \(p\) (the simple distribution) to be the canonical Gaussian. In order to satisfy the boundary conditions, we must have that \(u_0(\mathbf{x_1}) = 0\), \(\sigma_0(\mathbf{x_1}) = 1\), \(u_1(\mathbf{x_1}) = \mathbf{x_1}\) and \(\sigma_1(\mathbf{x_1}) = \sigma_{min}\).</p> <p>The simplest conditional flow that will generate \(p_t(\mathbf{x_1} \vert \mathbf{x_1})\) given that \(p\) is a canonical Gaussian is the following:</p> \[\phi_t(\mathbf{x} \vert\mathbf{x_1}) = \sigma_t(\mathbf{x_1})\mathbf{x} + u_t(\mathbf{x_1}),\] <p>where \(\mathbf{x} \sim p\). Indeed by example 1, this is true.</p> <p>The conditional vector field that generates this flow is given by the following:</p> \[u_t(\mathbf{x}\vert\mathbf{x_1}) = \frac{\sigma_t'(\mathbf{x_1})}{\sigma_t(\mathbf{x_1})}(\mathbf{x} - \mu_t(\mathbf{x_1})) + \mu'_t(\mathbf{x_1}).\] <p>In this setup, \(\mu_t\) is an arbitrary function that we can choose. Essentially, this allows us to select any arbitrary path from \(0\) to \(\mathbf{x_1}\). A natural choice for this is a straight line which is called the optimal transport solution.</p> <h4 id="optimal-transport">Optimal Transport</h4> <p>The optimal transport solution is the path that requires the least amount of work done to transform the canonical Gaussian to the mean \(u_t\) and std. \(\sigma_t\) Gaussian. Specifically, the mean and standard deviation change linearly with time:</p> \[u_t(\mathbf{x}) = t\mathbf{x_1}, \quad \text{and} \quad \sigma_t(\mathbf{x}) = 1 - (1 - \sigma_{min})t.\] <p>This straight line path is generated by the vector field:</p> \[u_t(\mathbf{x} \vert \mathbf{x_1}) = \frac{\mathbf{x_1} - (1 - \sigma_{min})\mathbf{x}}{1 - (1 - \sigma_{min})t}.\] <p>By substituting \(u_t\) and \(\sigma_t\), we get that the conditional flow in optimal transport case is:</p> \[\phi_t(\mathbf{x}|\mathbf{x_1}) = (1- (1 - \sigma_{min})t)\mathbf{x} + t\mathbf{x_1}.\] <p>Therefore, the reparameterized conditional flow matching loss is the following,</p> \[\mathbb{E}_{t, q(\mathbf{x_1}), p(\mathbf{x_0})}\left\lVert v_t(\phi_t(\mathbf{x_0}|\mathbf{x_1})) - (\mathbf{x_1} - (1 - \sigma_{min})\mathbf{x_0})\right\rVert^2.\] <p>The conditional flow is the optimal transport displacement map between two Gaussians. Although, the conditional flow is optimal it doesn’t imply that the marginal vector field is optimal.</p> <h3 id="riemannian-flow-matching-rfm">Riemannian Flow Matching (RFM)</h3> <p>In the previous section, we discussed how to do flow matching in \(\mathbb{R}^d\). Another interesting question is how do we do flow matching on non-Euclidean geometries? This is relevant if you already know that your data lies on a manifold.</p> <p><img src="./data.png" alt="alt-text"/> <em>Figure 2:</em> Consider a simple case where your data lies on a simple manifold in \(\mathbb{R}^2\) - the circle. Of course, on the left-hand side, you can use flow matching on Euclidean spaces to try to model this data. But it may be beneficial to specify as much prior knowledge you have about the data to obtain the best model. So performing flow matching on the manifold domain, the circle represented on the right, may lead to better performance.</p> <p>There are many real-world applications where we would want to model data that resides on a manifold. Examples include protein modelling, molecule modelling, robotics, medical imaging and geological sciences.</p> <p>In this section, we introduce Riemannian flow matching - a generalization of flow matching. Specifically, we consider complete, connected and smooth Riemannian manifolds, \(\mathcal{M}\) endowed with metric \(g\). Formally, we have a set of data samples \(\{x_i\}_{i=1}^N\) with \(x_i \in \mathcal{M}\) that arise from a probability distribution, \(q\) on \(\mathcal{M}\). We aim to learn a flow that transforms a simple noise distribution \(p\) on \(\mathcal{M}\) to the data distribution.</p> <p>The tangent space at \(x \in \mathcal{M}\) is denoted as \(T_x\mathcal{M}\). Also, \(g\) induces many key quantities. It defines an inner product over \(T_x\mathcal{M}\) denoted as \(\langle u,v \rangle _g\). We have the expontential map \(\exp_x: T_x\mathcal{M} \to \mathcal{M}\) and extensions of the gradient, divergence and Laplacian. For all \(x \in \mathcal{M}\), \(\text{div}_g{x}\) denotes the divergence with respect to the spatial (\(x\)) argument. The integration of the function \(f: \mathcal{M} \to \mathbb{R}\) is denotes as \(\int f(x) d\text{vol}_x\).</p> <p>Fortunately, there is not too many changes required to make flow matching work on manifolds. The objects used in RFM are the same as in FM. The space of probability densities over \(\mathcal{M}\) is defined as \(\mathcal{P}\). We have a probability path \(p_t: \mathcal{M} \times [0,1] \to \mathcal{P}\) such that \(\int p_t(x)d\text{vol}_x = 1\). The time dependent vector field is represented as \(u_t: \mathcal{M} \times [0,1] \to \mathcal{M}\). The flow \(\phi_t: \mathcal{M} \times [0,1] \to \mathcal{M}\) satisifies the following ODE defined on \(\mathcal{M}\):</p> \[\frac{d\phi_t(\mathbf{x})}{dt} = u_t(\phi_t(\mathbf{x})),\] <p>with initial condition \(\phi_0(\mathbf{x}) = \mathbf{x}\). The vector field \(u_t\) and probability path \(p_t\) also satisify the continuity equation on manifolds:</p> \[\frac{dp_t(\mathbf{x})}{dt} + \text{div}_g u_t(\phi_t(\mathbf{x})) = 0.\] <p>The vector field \(u_t\) generates the probability path \(p_t\) such that \(p_0 = p\) is the simple distribution and \(p_1 = q\) is the data distribution. The Riemannian flow matching objective is almost the same except we use \(g\) as the metric for the norm:</p> \[\mathcal{L}_{RFM}(\theta) = \mathbb{E}_{t, p_t(\mathbf{x})} \left\lVert v_t(\mathbf{x}) - u_t(\mathbf{x})\right\rVert^2_g.\] <p>Again, \(v_t\) is a learnable time-dependent vector field parameterized by \(\theta\). However, as before we don’t know the probability path \(p_t\) nor the vector field that generates this probability path. Since we cannot compute this loss, we use the Riemannian conditional flow matching loss instead.</p> <p>We condition on data samples to construct the conditional probability path and conditional vector field. Given \(\mathbf{x_1} \sim q\) we define the conditional path as \(p_t(\mathbf{x}\vert\mathbf{x_1})\) to satisfy the boundary conditions. As a note, we are keeping it general and not specifying the form of the conditional distribution. It does not have to be a Gaussian as in the Euclidean flow matching. Also, we can write the marginal probability path as</p> \[p_t(\mathbf{x}) = \int_{\mathcal{M}} p_t(\mathbf{x}\vert\mathbf{x_1})q(\mathbf{x_1})d\text{vol}_{\mathbf{x_1}}.\] <p>We define the conditional vector field \(u_t(\mathbf{x}\vert\mathbf{x_1})\) that generates this probability path. The marginal vector field can be obtained in a similar fashion as before:</p> \[u_t(x) = \int_{\mathcal{M}} u_t(x|\mathbf{x_1}) \frac{p_t(x|\mathbf{x_1})q(\mathbf{x_1})}{p_t(x)} d\text{vol}_{\mathbf{x_1}}.\] <p>Once again computing this integral is intractable which motivates us to define the Riemannian conditional flow matching loss:</p> \[\mathcal{L}_{RCFM}(\theta) = \mathbb{E}_{t, q(\mathbf{x_1}), p_t(\mathbf{x}\vert\mathbf{x_1})} ||v_t(\mathbf{x}) - u_t(\mathbf{x}\vert\mathbf{x_1})||^2_g.\] <p>We can reparameterize the loss as follows:</p> \[\mathcal{L}_{RCFM}(\theta) = \mathbb{E}_{t, q(\mathbf{x_1}), r(\mathbf{x_0})} \left\lVert v_t(\phi_t(\mathbf{x}\vert\mathbf{x_0})) - u_t(\phi(\mathbf{x} \vert \mathbf{x_0})\vert\mathbf{x_1})\right\rVert^2_g.\] <p>Now we need a way to construct the conditional flow. The conditional flow will map all points to \(\mathbf{x_1}\) at time \(t=1\) regardless of the choice of \(p\). So the flow satisfies:</p> \[\phi_1(\mathbf{x}\vert\mathbf{x_1}) = \mathbf{x_1}, \quad \forall \mathbf{x} \in \mathcal{M}.\] <p>Also, in the same manner in which we parameterized the loss function, we can sample \(\mathbf{x_0} \sim p\) and then compute \(\mathbf{x_t} = \phi_t(\mathbf{x_0} \vert \mathbf{x_1})\). Now, in order to construct the conditional flow, we consider two different cases. The first case is when we are on simple manifolds i.e. we have a closed form for the geodesics. Let \(d_g(\mathbf{x}, \mathbf{y})\) represent the geodesic distance between two points on the manifold. Let \(\kappa(t)\) be a monotonically decreasing function s.t. \(\kappa(0) = 1\) and \(\kappa(1) = 0\). We want to find a conditional flow \(\phi_t(\mathbf{x} \vert \mathbf{x_1})\) that will satisfy the following equation according to the scheduler \(\kappa\):</p> \[d_g(\phi_t(\mathbf{x_0} \vert \mathbf{x_1}), \mathbf{x_1}) = \kappa(t)d_g(\mathbf{x_0}, \mathbf{x_1}).\] <p>This will gaurantee that \(\phi_1(\mathbf{x} \vert \mathbf{x_1}) = \mathbf{x_1}\). A simple choice for this scheduler is \(\kappa(t) = 1 - t\). In fact, the conditional flow, \(\phi_t(\mathbf{x_0} \vert \mathbf{x_1})\) is a geodesic connecting \(x_0\) and \(x_1\). Additionally, the geodesic can be expressed as,</p> \[\phi_t(\mathbf{x_0} \vert \mathbf{x_1}) = \exp_{\mathbf{x_1}}(\kappa(t)\log_{\mathbf{x_1}}(\mathbf{x_0})),\] <p>which is simple to compute and results in a highly-scalable training objective. This conditional flow can be thought of as the analouge of interpolating between \(\mathbf{x_0}\) and \(\mathbf{x_1}\) in Euclidean space:</p> \[(1 - \kappa(t))\mathbf{x_1} + \kappa(t)\mathbf{x_0}.\] <p>When we are not on simple manifolds and don’t have access to the geodesic in closed form, we have to work with a pre-metric. A pre-metric is a function \(d: \mathcal{M} \times \mathcal{M} \to \mathbb{R}\) which satisfies the following properties:</p> <ul> <li>Non-negative: \(d(\mathbf{x}, \mathbf{y}) \geq 0\) for all \(x, y \in \mathcal{M}\)</li> <li>Positive: \(d(\mathbf{x}, \mathbf{y}) = 0\) iff \(x = y\)</li> <li>Non-degenerate: \(\nabla_x d(\mathbf{x}, \mathbf{y}) \neq 0\) iff \(x \neq y\)</li> </ul> <p>Note that a geodesic satisfies the definition for a premetric. Then we want a flow \(\phi_t(\mathbf{x_0} \vert \mathbf{x_1})\) to satisfy,</p> \[d(\phi_t(\mathbf{x_0} \vert \mathbf{x_1}), \mathbf{x_1}) = \kappa(t)d(\mathbf{x_0}, \mathbf{x_1}).\] <p>Once again, this will gaurantee that \(\phi_1(\mathbf{x_0} \vert \mathbf{x_1}) = \mathbf{x_1}\). Furthermore, the conditional vector field that generates this flow can be shown to be:</p> \[\mu_t(\mathbf{x} \vert \mathbf{x_1}) = \frac{d \log \kappa(t)}{dt} d(\mathbf{x}, \mathbf{x_1})\frac{\nabla_x d(\mathbf{x}, \mathbf{x_1})}{\lVert \nabla_x d(\mathbf{x}, \mathbf{x_1}) \rVert _g^2}.\] <p>Although this formula seems complicated, the basic component is the gradient of the distance, \(\nabla_x d(\mathbf{x}, \mathbf{x_1})\). This ensures we are going in the direction of \(\mathbf{x_1}\). The other terms control for the speed and make sure that the flow hits \(\mathbf{x_1}\) at time \(t=1\).</p> <p>If we don’t have access to the geodesic then there is no simple closed form interpolation like formula to compute \(\mathbf{x_t}\). Therefore, we must simulate/use an ODE solver to obtain \(\mathbf{x_t}\) which may computationally expensive.</p> <p>An example of a pre-metric is the spectral distance:</p> \[d_w(\mathbf{x},\mathbf{y})^2 = \sum_{i=1}^{\infty} w(\lambda_i) (\varphi_i(\mathbf{x}) - \varphi_i(\mathbf{y})),\] <p>where \(\varphi_i: \mathcal{M} \to \mathbb{R}\) are the eigenfunctions of the Laplace-Beltrami operator \(\Delta_g\) over \(\mathcal{M}\) with eigenvalues \(\lambda_i\), \(\Delta_g \varphi_i = \lambda_i \varphi_i\) and \(w: \mathbb{R} \to \mathbb{R}_{&gt;0}\) is some monotonically decreasing weighting function. Using the spectral distance can be more beneficial than geodesics because they are more robust to topological noise such as holes and shortcuts and are more geometry aware. An example of a spectral distance is the biharmonic distance which is helpful in avoiding boundaries of manifolds as show in the following figure.</p>]]></content><author><name>Robin Yadav</name></author><category term="generative"/><category term="models"/><summary type="html"><![CDATA[An introduction to flow matching.]]></summary></entry><entry><title type="html">Markov Chains</title><link href="https://dsl-lab.github.io/blog/2023/markov-chains/" rel="alternate" type="text/html" title="Markov Chains"/><published>2023-07-12T00:00:00+00:00</published><updated>2023-07-12T00:00:00+00:00</updated><id>https://dsl-lab.github.io/blog/2023/markov-chains</id><content type="html" xml:base="https://dsl-lab.github.io/blog/2023/markov-chains/"><![CDATA[<h1 id="markov-chains-definitions-and-representations">Markov Chains: Definitions and Representations</h1> <p>A stochastic process $X = { x(t): t\in T}$ is a collection of random variables.</p> <p>There are two elements:</p> <ul> <li>Time $t$: <ul> <li>discrete time ($T$ is a countably infinite set; under this case, we call ‘Markov chain’)</li> <li>continuous time (under this case, we call ‘Markov process’)</li> </ul> </li> <li>Space $\Omega$: <ul> <li>discrete space ($X_{t}$ comes from a countably infinite set)</li> <li>continuous space.</li> </ul> </li> </ul> <p>Markov chain is a <strong>discrete-time</strong> process for which the future behaviour, given the past and the present, only depends on the present and not on the past.</p> <p>Markov process is the <strong>continuous-time</strong> version of a Markov chain.</p> <blockquote> <p>Definition 1.[Markov chain] A discrete time stochastic process $ X_0, X_1, X_2, $. . . is a Markov chain if \(P(X_{t} = a_t | X_{t-1} = a_{t-1}, X_{t-2} = a_{t-2}, ..., X_0 = a_0) = P(X_{t} = a_t | X_{t-1} = a_{t-1}) = P_{a_{t-1}, a_{t}}\)</p> </blockquote> <p>Remark 1: This is time-homogeneous markov chain, for $\forall t$, for $\forall a_{t-1}, a_{t} \in \Omega$, the transition probability $P_{a_{t-1}, a_{t} }$ is the same.</p> <p>Remark 2: In DDPM, it is not a time-homogeneous chain, as the transition probability at t is obtained by a network(t).</p> <p>The state $X_{t}$ depends on the previous state $X_{t-1}$ but is independent of the particular history $X_{t-2}, X_{t-3},…$. This is called the <strong>Markov property</strong> or <strong>memoryless property</strong>.</p> <p>The Markov property does not imply that $X_{t}$ is independent of the random variables $X_{0}$, $X_{1}$,…, $X_{t-2}$; it just implies that <strong>any dependency of $X_{t}$ on the past is captured in the value of $X_{t-1}$</strong>.</p> <p>The Markov chain is <strong>uniquely</strong> defined by the one-step transition probability matrix P: \(P = \begin{pmatrix} P_{0,0} &amp; P_{0, 1} &amp; \cdots &amp; P_{0, j} &amp; \cdots\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\ P_{i,0} &amp; P_{i, 1} &amp; \cdots &amp; P_{i, j} &amp; \cdots\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\ \end{pmatrix}\) where $P_{i,j}$ is the probability of transition from state $i$ to state $j$. $P_{i,j} = P(X_{t} = j| X_{t-1} = i), i,j \in \Omega$. For \forall $i$, $\sum_{j \geq 0} P_{i,j} = 1$.</p> <h1 id="classification-of-states">Classification of States</h1> <p>For simplicity, we assume that the state space $\Omega$ is finite.</p> <h2 id="communicating-class">Communicating class</h2> <blockquote> <p>Definition 2. [Communicating class] A state $j$ is reachable from state $i$ if there exists a positive integer $n$ such that $P_{i,j}^{(n)} &gt; 0$. We write $i \rightarrow j$. If $j$ is reachable from $i$, and $i$ is reachable from $j$, then the states $i$ and $j$ are said to <strong>communicate</strong>, denoted by $i \leftrightarrow j$. A communicating class $C$ is a <strong>maximal</strong> set of states that communicate with each other. <strong>No state in $C$ communicates with any state not in $C$.</strong></p> </blockquote> <h2 id="irreducible">Irreducible</h2> <blockquote> <p>Definition 3: A Markov chain is <strong>irreducible</strong> if all states belong to <strong>one</strong> communicating class.</p> </blockquote> <p>This means that <strong>any state can be reached from any other state</strong>. For $\forall i, j \in \Omega$, $P_{i,j} &gt; 0$.</p> <blockquote> <p>Lemma 1. A finite Markov chain is irreducible if and only if its graph representation is a strongly connected graph.</p> </blockquote> <h3 id="transient-vs-recurrent-states">Transient vs Recurrent states</h3> <p>Let $r_{i,j}^{t}$ denote the probability that the chain, starting at state $i$, <strong>the first time</strong> transition to state $j$ occurs at time $t$. That is, \(r_{i,j}^{t} = P(X_{t} = j, X_{s} \neq j, \forall 1 \leq s \leq t-1 | X_{0} = i)\)</p> <blockquote> <p>Definition 4. A state is <strong>recurrent</strong> if $\sum_{t \geq 1} r_{i,i}^{t} = 1$ and it is <strong>transient</strong> if $\sum_{t \geq 1} r_{i,i}^{t} &lt; 1$. A Markov chain is recurrent if every state in the chain is recurrent.</p> </blockquote> <ul> <li> <p>If state i is recurrent then, once the chain visits that state, it will (with probability 1) eventually return to that state. Hence the chain will visit state $i$ over and over again, <strong>infinitely</strong> often.</p> </li> <li> <p>A transient state has the property that a Markov chain starting at this state returns to this state only <strong>finitely often</strong>, with probability 1.</p> </li> <li> <p>If one state in a communicating class is transient (respectively, recurrent) then all states in that class are transient (respectively, recurrent).</p> </li> </ul> <blockquote> <p>Definition 5. An irreducible Markov chain is called recurrent if at least one (equivalently, every) state in this chain is recurrent. An irreducible Markov chain is called transient if at least one (equivalently, every) state in this chain is transient.</p> </blockquote> <p>Let $\mu_{i} = \sum_{t \geq 1} t \cdot r_{i,i}^{t}$ denote the expected time to return to state $i$ when starting at state $i$.</p> <blockquote> <p>Definition 6. A state $i$ is <strong>positive recurrent</strong> if $\mu_{i} &lt; \infty$ and <strong>null recurrent</strong> if $\mu_{i} = \infty$.</p> </blockquote> <p>Here we give an example of a Markov chain that has null recurrent states. Consider the following markov chain whose states are the positive integers.</p> <p><img src="image.png" alt="Fig. 1. An example of a Markov chain that has null recurrent states "/></p> <p>Starting at state 1, the probability of not having returned to state 1 within the first $t$ steps is \(\prod_{j=1}^{t} \frac{j}{j+1} = \frac{1}{t+1}.\) The probability of never returning to state 1 from state 1 is 0, and state 1 is recurrent. Thus, the probability of the first time transition to state $1$ occurs at time $t$ is \(r_{1,1}^{t} = \frac{1}{t} \cdot \frac{1}{t+1} = \frac{1}{t(t+1)}.\) The expected number of steps until the first return to state 1 when starting at state 1 is \(\mu_{1} = \sum_{t = 1}^{\infty} t \cdot r_{1,1}^{t} = \sum_{t = 1}^{\infty} \frac{1}{t+1} = \infty.\) State 1 is recurrent but null recurrent.</p> <blockquote> <p>Lemma 2. In a finite Markov chain:</p> <ol> <li>at least one state is recurrent; and</li> <li>all recurrent states are positive recurrent.</li> </ol> </blockquote> <p>Thus, all states of a finite, irreducible Markov chain are positive recurrent.</p> <h3 id="periodic-vs-aperiodic-states">Periodic vs Aperiodic states</h3> <blockquote> <table> <tbody> <tr> <td>Definition 7. A state $j$ in a discrete time Markov chain is <strong>periodic</strong> if there exists an integer $k&gt;1$ such that $P(X_{t+s}= j</td> <td>X_t = j) = 0$ unless $s$ is divisible by $k$. A discrete time Markov chain is periodic if any state in the chain is periodic. A state or chain that is not periodic is <strong>aperiodic</strong>.</td> </tr> </tbody> </table> </blockquote> <table> <tbody> <tr> <td>A state $i$ is periodic means that for $s = k, 2k, 3k,…$, $P(X_{t+s}= j</td> <td>X_t = j) &gt; 0$.</td> </tr> </tbody> </table> <p><strong>NB: k &gt; 1</strong></p> <h3 id="ergodic">Ergodic</h3> <blockquote> <p>Definition 8. An <strong>aperiodic</strong>, <strong>positive recurrent</strong> state is an <strong>ergodic</strong> state. A Markov chain is ergodic if all its states are ergodic.</p> </blockquote> <blockquote> <p>Corollary 1. Any finite, irreducible, and aperiodic Markov chain is an ergodic chain.</p> </blockquote> <h3 id="stationary-distribution">Stationary distribution</h3> <p>Consider the two-state “broken printer” Markov chain:</p> <p><img src="2023-07-22-11-00-52.png" alt="Transition diagram for the two-state broken printer chain"/></p> <p>There are two state (0 and 1) in this Markov chain, and assume that the initial distribution is \(P(X_0 = 0) = \frac{\beta}{\alpha+\beta}, \qquad P(X_0 = 1) = \frac{\alpha}{\alpha+\beta}.\) Then, according to the transition probability matrix $P$, after one step, the distribution is \(\begin{align*} P(X_1 = 0) &amp;= P(X_0 = 0)P(X_1 = 0 | X_0 = 0) + P(X_0 = 1)P(X_1 = 0 | X_0 = 1) \\ &amp;= \frac{\beta}{\alpha+\beta} \cdot (1-\alpha) + \frac{\alpha}{\alpha+\beta} \cdot \beta = \frac{\beta}{\alpha+\beta}, \\ P(X_1 = 1) &amp;= P(X_0 = 0)P(X_1 = 1 | X_0 = 0) + P(X_0 = 1)P(X_1 = 1 | X_0 = 1) \\ &amp;= \frac{\beta}{\alpha+\beta} \cdot \alpha + \frac{\alpha}{\alpha+\beta} \cdot (1-\beta) = \frac{\alpha}{\alpha+\beta}. \end{align*}\) Apparently, the distribution of $X_1$ is the same as the initial distribution. Similarly, we can prove that the distribution of $X_t$ is the same as the initial distribution for any $t$. Here, $\pi = (\frac{\beta}{\alpha+\beta}, \frac{\alpha}{\alpha+\beta})$ is called <strong>stationary distribution</strong>.</p> <blockquote> <p>Definition 9. A probability distribution $\pi = (\pi_i)$, $\sum_{i \in \Omega} \pi_i = 1$(<strong>row vector</strong>) on the state space $\Omega$ is called a <strong>stationary distribution</strong> (or an equilibrium distribution) for the Markov chain with transition probability matrix $P$ if $\pi = \pi P$, equivalently, $\pi_j = \sum_{i \in \Omega}\pi_i P_{i,j}$ for all $j \in \Omega$.</p> </blockquote> <ul> <li> <p>One interpretation of the stationary distribution: if we started off a <strong>thousand</strong> Markov chains, choosing each starting position to be state $i$ with probability $\pi_i$, then(roughly) <strong>$1000 \pi_j$</strong> of them would be in state $j$ at any time in the future – but not necessarily the same ones each time.</p> </li> <li> <p>If a chain ever reaches a stationary distribution then it maintains that distribution for all future time, and thus a stationary distribution represents a steady state or an equilibrium in the chain’s behavior.</p> </li> </ul> <h4 id="finding-a-stationary-distribution">Finding a stationary distribution</h4> <p>Consider the following no-claims discount Markov chain with state space $\Omega = {1,2,3}$ and transition matrix \(P = \begin{pmatrix} \frac{1}{4} &amp; \frac{3}{4} &amp; 0\\ \frac{1}{4} &amp; 0 &amp; \frac{3}{4}\\ 0 &amp; \frac{1}{4} &amp; \frac{3}{4} \end{pmatrix}\)</p> <ul> <li> <p>Step 1: Assume $\pi = {\pi_1, \pi_2, \pi_3} $ is a stationary distribution. According to the definition 9 of stationary distribution, we need to solve the following equations: \(\begin{align*} \pi_1 &amp;= \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2, \\ \pi_2 &amp;= \frac{3}{4}\pi_1 + \frac{1}{4}\pi_3, \\ \pi_3 &amp;= \frac{3}{4}\pi_2 + \frac{3}{4}\pi_3. \end{align*}\) Adding the normalising condition $\pi_1 + \pi_2 + \pi_3 = 1$, we get four equations in three unknown parameters.</p> </li> <li>Step 2: Choose one of the parameters, say $\pi_1$, and solve for the other two parameters in terms of $\pi_1$. We get \(\pi_1 = \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2 \Rightarrow \pi_2 = 3\pi_1, \qquad \pi_3 = 3\pi_2 = 9\pi_1.\)</li> <li>Step 3: Combining with the normalising condition, we get \(\pi_1 + 3\pi_1 + 9\pi_1 = 1 \Rightarrow \pi_1 = \frac{1}{13}, \qquad \pi_2 = \frac{3}{13}, \qquad \pi_3 = \frac{9}{13}.\) Finally, we get the stationary distribution $\pi = (\frac{1}{13}, \frac{3}{13}, \frac{9}{13})$.</li> </ul> <h4 id="existence-and-uniqueness">Existence and uniqueness</h4> <p>Given a Markov chaine, how can we know whether it has a stationary distribution? If it has, is it unique? At this part, we will answer these questions.</p> <p>Some notations:</p> <ul> <li> <p>Hitting time to hit the state $j$: $H_{j} = \min { t \in {0, 1, 2,…}: X_t = j}$. Note that here we include time $t = 0$.</p> </li> <li> <table> <tbody> <tr> <td>Hitting probability to hit the state $j$ staring from state $i$: $h_{i,j} = P(X_t = j, \text{for some} \ t \geq 0</td> <td>X_0 = i) = P(H_{j} &lt; \infty</td> <td>X_0 = i) = \sum_{t \geq 0} r_{i,j}^{t}$.</td> </tr> </tbody> </table> </li> </ul> <p>Note that this is different from $r_{i,j}^{t}$, which denotes the probability that the chain, starting at state $i$, the <strong>first</strong> time transition to state $j$ <strong>occurs at time $t$</strong>.</p> <p>We also have \(h_{i,j} = \begin{cases} \sum_{k \in \Omega}P_{i,k}h_{k,j} &amp; , &amp; \text{if} \quad j \ne i, \\ 1 &amp; , &amp; \text{if} \quad j = i. \end{cases}\)</p> <ul> <li> <p>Expected hitting time: $\eta_{i,j} = E(H_{j} | X_0 = i) = \sum_{t \geq 0} t \cdot r_{i,j}^{t}$. The expected time until we hit state $j$ starting from state $i$. We also have \(\eta_{i,j} = \begin{cases} 1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j} &amp; , &amp; if j \ne i, \\ 0 &amp; , &amp; if j = i. \end{cases}\) (For the first case, we add 1 because we need to consider the first step from state $i$ to state $k$.)</p> </li> <li>Return time: $M_i = \min { t \in {1, 2,…}: X_t = i}$. It is different from $H_{i}$, as we exclude time $t = 0$. It is the first time that the chain returns to state $i$ after $t = 0$.</li> <li> <table> <tbody> <tr> <td>Return probability: $m_{i} = P(X_t = i \ \text{for some} \ n \geq 1</td> <td>X_0 = i) = P(M_i &lt; \infty</td> <td>X_0 = i) = \sum_{t&gt;1}r_{i,i}^{t}.$</td> </tr> </tbody> </table> </li> <li>Expected return time: $\mu_{i} = E(M_i | X_0 = i) = \sum_{t \geq 1} t \cdot r_{i,i}^{t}$. The expected time until we return to state $i$ starting from state $i$. \(m_{i} = \sum_{j \in \Omega} P_{i,j}h_{j,i}, \qquad \mu_{i} = 1 + \sum_{j \in \Omega} P_{i,j}\eta_{j,i}.\)</li> </ul> <hr/> <blockquote> <p>Theorem 1. Consider an irreducible Markov chain (<strong>finite or infinite</strong>), (1) if it is <strong>positive recurrent</strong>, $\exists$ an unique stationary distribution $\pi$, such that $\pi_i = \frac{1}{\mu_{i}}$. (2) if it is <strong>null recurrent</strong> or <strong>transient</strong>, no stationary distribution exists.</p> </blockquote> <p>Remark: If the chain is <strong>finite</strong> irreducible, it must be positive recurrent, thus it has an unique stationary distribution.</p> <p>Remark: If the Markov chain is not irreducible, we can decompose the state space into several communicating classes. Then, we can consider each communicating class separately.</p> <ul> <li>If none of the classes are positive recurrent, then no stationary distribution exists.</li> <li>If exactly one of the classes is positive recurrent (and therefore closed), then there exists a unique stationary distribution, supported only on that closed class.</li> <li>If more the one of the classes are positive recurrent, then many stationary distributions will exist.</li> </ul> <p>Now, we give the proof of Theorem 1. We first prove that if a Markov chain is irreducible and positive recurrent, then there <strong>exists</strong> a stationary distribution. Next, we will prove the stationary distribution is <strong>unique</strong>. Since the second part with the null recurrent or transitive Markov chains is less important and more complicated, we will omit it. If you are interested in it, you can refer to the book <a href="https://www.statslab.cam.ac.uk/~james/Markov/">Markov Chains</a> by James Norris.</p> <p>Proof. (1) Suppose that $(X_0, X_1 …)$ a recurrent Markov chain, which can be positive recurrent or null recurrent. Then we can desigh a stationary distribution as follows. (If we can desigh a stationary distribution, then it must be existed.)</p> <p>Let $\nu_i$ be the expected number of visits to $i$ before we return back to $k$, \(\begin{align*} \nu_i &amp;= \mathbb{E}(\# \text{visits to $i$ before returning to } k | X_0 = k) \\ &amp;= \mathbb{E}\sum_{t=1}^{M_k} P(X_t = i | X_0 = k) \\ &amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1} P(X_t = i | X_0 = k) \end{align*}\) The last equation holds because of $ P(X_0 = i | X_0 = k) = 0$ and $ P(X_{M_k} = i | X_0 = k) = 0$.</p> <p>If we want design a stationary distribution, it must statisfy $\pi P = \pi$ and $\sum_{i \in \Omega}\pi_i = 1$.</p> <p>(a) We first prove that $\nu P = \nu$. \(\begin{align*} \sum_{i \in \Omega} \nu_i P_{i,j} &amp;= \mathbb{E}\sum_{i \in \Omega} \sum_{t = 0}^{M_k - 1} P(X_t = i, X_{t+1} = j | X_0 = k) \\ &amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1} \sum_{i \in \Omega} P(X_t = i, X_{t+1} = j | X_0 = k) \\ &amp;= \mathbb{E} \sum_{t = 0}^{M_k - 1} P(X_{t+1} = j | X_0 = k) \\ &amp;= \mathbb{E} \sum_{t = 1}^{M_k } P(X_{t} = j | X_0 = k) \\ &amp;= \mathbb{E} \sum_{t = 0}^{M_k - 1} \nu_i \\ &amp;= \nu_j. \end{align*}\) (b) Next, what we need to do is to normalize $\nu$ to get a stationary distribution. We have \(\sum_{i \in \Omega} \nu_i = \sum_{i \in \Omega} \mathbb{E} \sum_{t = 0}^{M_k - 1} P(X_t = i | X_0 = k) =\mathbb{E} \sum_{t = 0}^{M_k - 1} \sum_{i \in \Omega} P(X_t = i | X_0 = k) = E(M_k | X_0 = i) = \mu_k.\) Thus, we can define $\pi_i = \nu_i/\mu_k$, $\pi = {\pi_i, i \in \Omega}$ is one of the stationary distribution.</p> <p>(2) Next, we prove that if a Markov chain is irreducible and positive recurrent, then the stationary distribution is <strong>unique</strong> and is given by $\pi_j = \frac{1}{\mu_j}$.</p> <p>Given a stationary distribution $\pi$, if we prove that for all $i$, $\pi_j == \frac{1}{\mu_j}$, then we prove that the stationary distribution is unique.</p> <p>Remember that the expected hitting time: \(\eta_{i,j} = 1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j}, j \ne i \qquad (eq:1)\) We multiply both sides of (eq:1) by $\pi_i$ and sum over $i (i \ne j)$ to get \(\sum_{i \ne j} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i \ne j} \sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j}\) Since $\eta_{j,j} = 0$, we can rewrite the above equation as \(\sum_{i \in \Omega} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i \ne j} \sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j}. \qquad (eq:2)\)</p> <p>(The above equality lacks $j$, and we also want to design $\pi_j = 1/\mu_j$.) Remember that the expected return time: \(\mu_{j} = 1 + \sum_{i \in \Omega} P_{j,i}\eta_{i,j}. \qquad (eq:3)\) We multiply both sides of (eq:2) by $\pi_j$ to get \(\pi_j \mu_{j} =\pi_j + \sum_{k \in \Omega} \pi_j P_{j,k}\eta_{k,j} \qquad (eq:4)\) Adding (eq:2) and (eq:4), we get \(\begin{align*} \sum_{i \in \Omega} \pi_i \eta_{i,j} + \pi_j \mu_{j} &amp;= \sum_{i \in \Omega} \pi_i + \sum_{i \in \Omega} \sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j} \\ &amp;= 1 + \sum_{k \in \Omega} \sum_{i \in \Omega} \pi_i P_{i,k}\eta_{k,j} \\ &amp;= 1 + \sum_{k \in \Omega} \pi_k \eta_{k,j} \qquad (\text{since} \sum_{i \in \Omega} \pi_i P_{i,k} = \pi_k) \\ \end{align*}\) <strong>Since the Markov chain is irreducible and positive recurrent, that means all states belong to a communication class and the expected return time of each state is finite. Thus, the space $\Omega$ is a finite dimensional space.</strong> We can substract $\sum_{k \in \Omega} \pi_k \eta_{k,j}$ and $\sum_{i \in \Omega} \pi_i \eta_{i,j} $ (equal) from both sides of the above equation to get \(\pi_j \mu_{j}=1,\) which means $\pi_j = 1/\mu_j$. Similarly, we can prove that $\pi_i = 1/\mu_i$ for all $i \in \Omega$.</p> <hr/> <blockquote> <p>Theorem 2 (Limit theorem) Consider an irreducible, aperiodic Markov chain (maybe infinite), we have $\lim\limits_{t \to \infty} P_{i,j}^{t} = \frac{1}{\mu_{j}}$. Spectially, (1) Suppose the Markov chain is positive recurrent. Then $\lim\limits_{t \to \infty} P_{i,j}^{t} = \pi_j = \frac{1}{\mu_{j}}$. (2) Suppose the Markov chain is null recurrent or transient. Then there is no limite probability.</p> </blockquote> <ul> <li>Three conditions for convergence to an equilibrium probability distribution: irreducibility, aperiodicity, and positive recurrence. The limit probability \(P = \begin{pmatrix} \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\ \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\ \end{pmatrix}\) where each row is identical.</li> </ul> <hr/> <p>Define $V_{i,j}^{t} = |{ n &lt; t | X_n = j}|$. $V_{i,j}^{t}$ is the number of visits to state $j$ before time $t$ starting from state $i$. Then we can interpret $V_{i,j}^{t}/t$ as the proportion of time up to time $t$ spent in state $j$.</p> <blockquote> <p>Theorem 3 [Ergodic theorem] Consider an irreducible Markov chain, we have $\lim\limits_{t \to \infty} V_{i,j}^{t}/t = \frac{1}{\mu_{j}}$ <strong>almost surely</strong>. Spectially, (1) Suppose the Markov chain is positive recurrent. Then $\lim\limits_{t \to \infty} V_{i,j}^{t}/t = \pi_j = \frac{1}{\mu_{j}}$ <strong>almost surely</strong>. (2) Suppose the Markov chain is null recurrent or transient. Then $ V_{i,j}^{t}/t \to 0$ <strong>almost surely</strong> for all $j$.</p> </blockquote> <p><strong>almost surely</strong> means that the convergence probability of the event is 1.</p> <hr/> <blockquote> <p>Theorem 4[Detailed balance condition]. Consider a finite, irreducible, and ergodic Markov chain with transition matrix $P$. If there are nonnegative numbers $\bar{\pi} = (\pi_0, \pi_1, …, \pi_n)$ such that $\sum_{i=0}^{n} \pi_i = 1$ and if, for any pair of states $i, j$, \(\pi_i P_{i,j} = \pi_{j} P_{j,i},\) then $\bar{\pi}$ is the stationary distribution corresponding to $P$.</p> </blockquote> <p>Proof. \(\sum_{i} \pi_i P_{i,j} = \sum_{i}\pi_{j} P_{j,i} = \pi_{j}\) Thus, $\bar{\pi} = \bar{\pi}P$. Since this is a finite, irreducible, and ergodic Markov chain, $\bar{\pi}$ must be the unique stationary distribution of the Markov chain.</p> <p>Remark: Theorem 2 is a sufficient but not necessary condition.</p> <h2 id="reference">Reference</h2> <ul> <li>Mitzenmacher, M., &amp; Upfal, E. (2005). Probability and Computing. Cambridge University Press.</li> <li><a href="https://mpaldridge.github.io/math2750/S09-recurrence-transience.html">Recurrence and transience</a></li> <li><a href="https://mpaldridge.github.io/math2750/S07-classes.html">Class structure</a></li> <li><a href="https://mpaldridge.github.io/math2750/S10-stationary-distributions.html">Stationary distributions</a></li> <li>Stirzaker, David. <a href="https://www.ctanujit.org/uploads/2/5/3/9/25393293/_elementary_probability.pdf">Elementary Probability</a></li> </ul>]]></content><author><name>Xue Yu</name></author><category term="probability,"/><category term="random"/><category term="process,"/><category term="Markov"/><category term="Chains"/><summary type="html"><![CDATA[An introduction to Markov Chains.]]></summary></entry></feed>