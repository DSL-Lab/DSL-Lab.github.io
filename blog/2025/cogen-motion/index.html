<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Conditional Generative Models for Motion Prediction | Laboratory for Deep Structured Learning</title> <meta name="author" content=" "> <meta name="description" content="In this blog post, we discuss good engineering practices and the lessons learned—sometimes the hard way—from building conditional generative models (in particular, flow matching) for motion prediction problems."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://dsl-lab.github.io/blog/2025/cogen-motion/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Conditional Generative Models for Motion Prediction",
      "description": "In this blog post, we discuss good engineering practices and the lessons learned—sometimes the hard way—from building conditional generative models (in particular, flow matching) for motion prediction problems.",
      "published": "August 17, 2025",
      "authors": [
        {
          "author": "Qi Yan",
          "authorURL": "https://qiyan98.github.io/",
          "affiliations": [
            {
              "name": "UBC",
              "url": ""
            }
          ]
        },
        {
          "author": "Yuxiang Fu",
          "authorURL": "https://felix-yuxiang.github.io/",
          "affiliations": [
            {
              "name": "UBC",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Laboratory for Deep Structured Learning</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blogs<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/members/">members</a> </li> <li class="nav-item "> <a class="nav-link" href="/photo/">photos</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Conditional Generative Models for Motion Prediction</h1> <p>In this blog post, we discuss good engineering practices and the lessons learned—sometimes the hard way—from building conditional generative models (in particular, flow matching) for motion prediction problems.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> <div><a href="#challenges-of-multi-modal-prediction">Challenges of Multi-Modal Prediction</a></div> <div><a href="#engineering-practices-and-lessons">Engineering Practices and Lessons</a></div> <ul> <li><a href="#data-space-predictive-learning-objectives">Data-Space Predictive Learning Objectives</a></li> <li><a href="#joint-multi-modal-learning-losses">Joint Multi-Modal Learning Losses</a></li> </ul> <div><a href="#exploring-inference-acceleration">Exploring Inference Acceleration</a></div> <div><a href="#summary">Summary</a></div> </nav> </d-contents> <h2 id="introduction">Introduction</h2> <p>Needless to say, diffusion-based generative models (equivalently, flow matching models) are amazing inventions. They have shown great capacity to produce high-quality images, videos, audios and more, whether being unconditional on the benchmark datasets and conditioned on certain content in the wild. In this blog, we discuss a relatively less explored application of <strong>generative models for motion prediction</strong>, which is a fundamental problem in many applications such as autonomous driving and robotics.</p> <p>In a nutshell, motion prediction is the task of predicting the future trajectories of objects given their past trajectories, plus all sorts of available context information such as surrounding objects and high-fidelity maps. <br> The said pipeline implemented by neural networks is simply:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Past Trajectory + Context Information ---&gt; Neural Network ---&gt; Future Trajectory
</code></pre></div></div> <p>To produce meaningful future trajectories, we condition the generative models on the past trajectory and the context information. Borrowed from our paper <d-cite key="fu2025moflow"></d-cite>, the pipeline looks like this:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2025/cogen-motion/noise_to_traj_moflow-480.webp 480w,/blog/2025/cogen-motion/noise_to_traj_moflow-800.webp 800w,/blog/2025/cogen-motion/noise_to_traj_moflow-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/blog/2025/cogen-motion/noise_to_traj_moflow.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The pipeline of motion prediction using conditional (denoising) generative models <d-cite key="fu2025moflow"></d-cite>. </div> <p>The early datasets on human motion prediction mostly do not come with heavy context information, such as the well-known ETH-UCY and the SDD datasets (see more for summarization at <d-cite key="ivanovic2023trajdata"></d-cite>), which the above figure accurately depicts. However, modern industry-standard datasets such as the Waymo Open Motion Dataset <d-cite key="ettinger2021large"></d-cite> and the Argoverse series datasets <d-cite key="wilson2023argoverse, chang2019argoverse"></d-cite> come with much richer context information, such as high-fidelity maps and other rich context information, which need more compute to process. No matter how complex the context information is, the generative model must be guided to <strong>produce spatially and temporally coherent trajectories consistent with the past</strong>.</p> <h2 id="challenges-of-multi-modal-prediction">Challenges of Multi-Modal Prediction</h2> <p>Motion <em>prediction</em>, as the name suggests, is inherently a forecasting task. For each input in a dataset, only one realization of the future motion is recorded, even though multiple plausible outcomes often exist. This mismatch between the inherently <strong>multi-modal</strong> nature of future motion and the <strong>single ground-truth</strong> annotation poses a core challenge for evaluation.</p> <p>In practice, standard metrics require models to output multiple trajectories, which are then compared against the observed ground truth. For example, <strong>ADE (Average Displacement Error)</strong> and <strong>FDE (Final Displacement Error)</strong> measure trajectory errors, and the minimum ADE/FDE across predictions is typically reported. This setup implicitly encourages models to produce diverse hypotheses, but only rewards the one closest to the recorded future. Datasets such as Waymo Open Motion <d-cite key="ettinger2021large"></d-cite> and Argoverse <d-cite key="wilson2023argoverse, chang2019argoverse"></d-cite> extend evaluation with metrics targeting uncertainty calibration. For instance, Waymo’s <strong>mAP</strong> rewards models that assign higher confidence to trajectories closer to the ground truth.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2025/cogen-motion/vehicle_1_trajflow-480.webp 480w,/blog/2025/cogen-motion/vehicle_1_trajflow-800.webp 800w,/blog/2025/cogen-motion/vehicle_1_trajflow-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/blog/2025/cogen-motion/vehicle_1_trajflow.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Multi-modal trajectory forecasting made by TrajFlow <d-cite key="yan2025trajflow"></d-cite> on the Waymo Open Motion Dataset <d-cite key="ettinger2021large"></d-cite>. Multiple predictions are visualized using different colors, while the single ground truth is shown in red. </div> <p>The strong dependency of current evaluation metrics on a single ground truth, assessed instance by instance, poses a particular challenge for generative models. Although the task inherently requires generating diverse trajectories, models are only rewarded when one of their outputs happens to align closely with the recorded ground truth.</p> <p>As a result, the powerful ability of generative models to produce diverse samples from noise <d-cite key="ho2020denoising, lipman2022flow"></d-cite> does not necessarily translate into better performance under current metrics. For example, MotionDiffuser <d-cite key="jiang2023motiondiffuser"></d-cite>, a diffusion-based model that generates one trajectory at a time, requires a complex post-processing pipeline—ranging from likelihood-based filtering to hand-crafted attractor/repeller cost functions and non-maximum suppression (NMS) for outlier removal—in order to achieve reasonably good results.</p> <h2 id="engineering-practices-and-lessons">Engineering Practices and Lessons</h2> <p>Now let’s dive into the technical side of the problem. In the forward process of flow matching, we adopt a simple linear interpolation between the clean trajectories \(Y^1 \sim q\), where \(q\) is the data distribution, and pure Gaussian noise \(Y^0 \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\):</p> \[Y^t = (1-t)Y^0 + tY^1 \qquad t \in [0, 1].\] <p>The reverse process, which allows us to generate new samples, is governed by the ordinary differential equations (ODEs):</p> \[\mathrm{d} Y^t = v_\theta(Y^t, t, C)\mathrm{d}t,\] <p>where \(v_\theta\) is the parametrized vector field approximating the straight flow \(U^t = Y^1 - Y^0\). Here, \(C\) denotes the aggregated contextual information of agents in a scene, including the past trajectory and any other available context information.</p> <h3 id="data-space-predictive-learning-objectives">Data-Space Predictive Learning Objectives</h3> <p>From an engineering standpoint, a somewhat <strong>bitter lesson</strong> we encountered is that <strong>existing predictive learning objectives remain remarkably strong</strong>. Despite the appeal of noise-prediction formulations (e.g., $\epsilon$-prediction introduced in DDPM <d-cite key="ho2020denoising"></d-cite> and later adopted in flow matching <d-cite key="lipman2022flow"></d-cite>), straightforward predictive objectives in the data space—such as direct \(\hat{x}_0\) reconstruction in DDPM notation<d-footnote> Note that we follow the flow matching notations in <d-cite key="lipman2022flow"></d-cite> to use $t=1$ as the data distribution and $t=0$ as the noise distribution, which is opposite to the original DDPM notations in <d-cite key="ho2020denoising"></d-cite>.</d-footnote>—consistently yields more stable convergence.</p> <p>Concretely, by rearranging the original linear flow objective, we define a neural network</p> \[D_\theta := Y^t + (1-t)v_\theta(Y^t, C, t),\] <p>which is trained to recover the future trajectory \(Y^1\) in the data space. The corresponding objective is:</p> \[\mathcal{L}_{\text{FM}} = \mathbb{E}_{Y^t, Y^1 \sim q, \, t \sim \mathcal{U}[0,1]} \left[ \frac{\| D_{\theta}(Y^t, C, t) - Y^1 \|_2^2}{(1 - t)^2} \right].\] <p>Our empirical observation is that data-space predictive learning objectives outperform denoising objectives. We argue that this is largely influenced by the current evaluation protocol, which heavily rewards model outputs that are close to the ground truth.</p> <p>During training, the original denoising target matches the vector field $Y^1 - Y^0$, defined as the difference between the data sample (future trajectory) and the noise sample (drawn from the noise distribution). Under the current proximity-based metrics, this objective is harder to optimize than the predictive objective because of the stochasticity introduced by $Y^0$, as the metrics do not adequately reward diverse forecasting. Moreover, during the sampling process, small errors in the vector field model $v_\theta$—measured with respect to the single ground-truth velocity field at intermediate time steps—can be amplified through subsequent iterative steps. Consequently, increasing inference-time compute may not necessarily improve results without incorporating regularization from the data-space loss <d-footnote> Interestingly, in our experiments, we found that flow-matching ODEs—thanks to their less noisy inference process—usually perform more stably than diffusion-model SDEs, which is surprising. In image generation, as shown in SiT <d-cite key="ma2024sit"></d-cite>, ODE-based samplers are generally weaker than SDE-based samplers. </d-footnote>.</p> <h3 id="joint-multi-modal-learning-losses">Joint Multi-Modal Learning Losses</h3> <p>Building on this, another key engineering practice was to introduce <strong>joint multi-modal learning losses</strong>. Our network \(D_\theta\) generates \(K\) scene-level correlated waypoint predictions \(\{S_i\}_{i=1}^K\) along with classification logits \(\{\zeta_i\}_{i=1}^K\)<d-footnote> Usually, different datasets have different conventions for what a proper $K$ should be. For example, $K=20$ is used for the ETH-UCY dataset, while $K=6$ is used for the Waymo Open Motion Dataset <d-cite key="ettinger2021large"></d-cite>. </d-footnote>. This allows us to capture diverse futures in a single inference loop while still grounding learning in a predictive loss. Such a principle of combined regression and classification losses to encourage trajectory multi-modality is ubiquitous in the motion prediction literature, as seen in MTR <d-cite key="shi2022motion"></d-cite>, UniAD <d-cite key="hu2023planning"></d-cite>, and QCNet <d-cite key="zhou2023query"></d-cite>, though these methods differ in other implementation details. For simplicity, we omit the time-dependent weighting and define the multi-modal flow matching loss:</p> \[\bar{\mathcal{L}}_{\text{FM}} = \mathbb{E}_{Y^t, Y^1 \sim q, \, t \sim \mathcal{U}[0,1]} \left[ \| S_{j^*} - Y^1 \|_2^2 + \text{CE}(\zeta_{1:K}, j^*) \right],\] <p>where \(j^* = \arg\min_{j} \| S_j - Y^1 \|_2^2\) indicates the closest waypoint to the ground-truth trajectory and \(\text{CE}(\cdot,\cdot)\) denotes cross-entropy. On tasks where confidence calibration is important, such as those measured by the mAP metric in the Waymo Open Motion Dataset, we refer readers to our paper <d-cite key="yan2025trajflow"></d-cite> for further details on uncertainty calibration.</p> <p>We acknowledge that some prior works, such as MotionLM <d-cite key="seff2023motionlm"></d-cite> and MotionDiffuser <d-cite key="jiang2023motiondiffuser"></d-cite>, generate one trajectory at a time and have demonstrated strong performance. However, since these methods are not open-sourced, we are unable to conduct direct comparisons or measure their runtime efficiency. We conjecture that requiring multiple inference loops (tens to hundreds) is considerably slower than our one-step generator—particularly on smaller-scale datasets, where the one-step approach achieves comparable performance without significant degradation.</p> <h2 id="exploring-inference-acceleration">Exploring Inference Acceleration</h2> <p>To accelerate inference in flow-matching models, which typically require tens or even hundreds of iterations for ODE simulation, we adopt an underrated idea from the image generation literature: conditional <strong>IMLE (implicit maximum likelihood estimation)</strong> <d-cite key="li2018implicit, li2019diverse"></d-cite>. IMLE provides a way to distill an iterative generative model into a <strong>one-step generator</strong>.</p> <p>The IMLE family consists of generative models designed to produce diverse samples in a single forward pass, conceptually similar to the generator in GANs <d-cite key="goodfellow2020generative"></d-cite>. In our setting, we construct a conditional IMLE model that takes the same context \(C\) as the teacher flow-matching model and learns to match the teacher’s motion prediction results directly in the data space.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2025/cogen-motion/imle_moflow-480.webp 480w,/blog/2025/cogen-motion/imle_moflow-800.webp 800w,/blog/2025/cogen-motion/imle_moflow-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/blog/2025/cogen-motion/imle_moflow.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Pipeline of the IMLE distillation process in our work <d-cite key="fu2025moflow"></d-cite>. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2025/cogen-motion/IMLE_algorithm-480.webp 480w,/blog/2025/cogen-motion/IMLE_algorithm-800.webp 800w,/blog/2025/cogen-motion/IMLE_algorithm-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/blog/2025/cogen-motion/IMLE_algorithm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The IMLE distillation process is summarized in <code class="language-plaintext highlighter-rouge">Algorithm 1</code>. Lines 4–6 describe the standard ODE-based sampling of the teacher model, which produces $K$ correlated multi-modal trajectory predictions \(\hat{Y}^1_{1:K}\) conditioned on the context $C$. A conditional IMLE generator $G_\phi$ then uses a noise vector $Z$ and context $C$ to generate $K$-component trajectories $\Gamma$, matching the shape of \(\hat{Y}^1_{1:K}\).</p> <p>Unlike direct distillation, the conditional IMLE objective generates <strong>more</strong> samples than those available in the teacher’s dataset for the same context $C$. Specifically, $m$ i.i.d. samples are drawn from $G_\phi$, and the one closest to the teacher prediction \(\hat{Y}^1_{1:K}\) is selected for loss computation. This nearest-neighbor matching ensures that the teacher model’s modes are faithfully captured.</p> <p>To preserve trajectory multi-modality, we employ the Chamfer distance <d-cite key="fan2017point"></d-cite> $d_{\text{Chamfer}}(\hat{Y}^1, \Gamma)$ as our loss function:</p> \[\mathcal{L}_{\text{IMLE}}(\hat{Y}^1_{1:K}, \Gamma) = \dfrac{1}{K} \left( \sum_{i=1}^K \min_j \|\hat{Y}^1_i - \Gamma^{(j)}\| + \sum_{j=1}^K \min_i \|\hat{Y}^1_i - \Gamma^{(j)}\| \right)\] <p>where $\Gamma^{(i)} \in \mathbb{R}^{A \times 2T_f}$ is the $i$-th component of the IMLE-generated correlated trajectory.</p> <p>Nonetheless, the acceleration of diffusion-based models—particularly through distillation—is evolving rapidly. Our work with IMLE is just one attempt in this direction, and we are actively exploring further improvements to extend its applicability to broader domains.</p> <h2 id="summary">Summary</h2> <p>We reviewed the challenges and engineering insights gained from developing conditional generative models for motion prediction, primarily drawing on our previous works <d-cite key="fu2025moflow, yan2025trajflow"></d-cite>. The task requires generating diverse trajectories, yet common evaluation metrics such as ADE and FDE primarily reward alignment with a single ground-truth trajectory.</p> <p>From these experiences, we identified two useful engineering practices:</p> <ul> <li>Data-space predictive learning objectives outperform denoising-based approaches, leading to more stable convergence.</li> <li>Joint multi-modal learning losses that integrate regression and classification more effectively capture trajectory diversity.</li> </ul> <p>In addition, we explored the IMLE distillation technique to accelerate inference by compressing iterative processes into a one-step generator, while preserving multi-modality through Chamfer distance losses.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2025-08-17-cogen-motion.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JGPDT2N8BY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JGPDT2N8BY");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>