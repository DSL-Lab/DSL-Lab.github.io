<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Markov Chains | Laboratory for Deep Structured Learning</title> <meta name="author" content=" "> <meta name="description" content="An introduction to Markov Chains."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://dsl-lab.github.io/blog/2023/markov-chains/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams",inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]]},options:{skipHtmlTags:["script","noscript","style","textarea","pre"],ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"},startup:{ready:()=>{MathJax.startup.defaultReady(),MathJax.startup.promise.then(()=>{console.log("MathJax initial typesetting complete")})}}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Markov Chains",
      "description": "An introduction to Markov Chains.",
      "published": "July 12, 2023",
      "authors": [
        {
          "author": "Xue Yu",
          "authorURL": "https://openreview.net/profile?id=~Xue_Yu2",
          "affiliations": [
            {
              "name": "Renmin University of China and UBC",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Laboratory for Deep Structured Learning</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blogs<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/members/">members</a> </li> <li class="nav-item "> <a class="nav-link" href="/photo/">photos</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Markov Chains</h1> <p>An introduction to Markov Chains.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#markov-chains-definitions-and-representations">Markov Chains - Definitions and Representations</a></div> <div><a href="#classification-of-states">Classification of States</a></div> <ul> <li><a href="#communicating-class">Communicating class</a></li> <li><a href="#irreducible-markov-chain">Irreducible Markov chain</a></li> <li><a href="#transient-vs-recurrent-states">Transient vs recurrent states</a></li> <li><a href="#periodic-vs-aperiodic-states">Periodic vs aperiodic states</a></li> <li><a href="#ergodicity">Ergodicity</a></li> <li><a href="#stationary-distribution">Stationary distribution</a></li> <li><a href="#finding-a-stationary-distribution">Finding a stationary distribution</a></li> <li><a href="#existence-and-uniqueness">Existence and uniqueness</a></li> </ul> <div><a href="#theorems-on-irreducible-markov-chains">Theorems on Irreducible Markov Chains</a></div> <ul> <li><a href="#existence-and-uniqueness-of-stationary-distribution">Existence and uniqueness of stationary distribution</a></li> <li><a href="#limit-dheorem">Limit dheorem</a></li> <li><a href="#ergodic-dheorem">Ergodic dheorem</a></li> <li><a href="#detailed-balance-condition">Detailed balance condition</a></li> </ul> <div><a href="#further-reading">Further Reading</a></div> </nav> </d-contents> <h2 id="markov-chains-definitions-and-representations">Markov Chains: Definitions and Representations</h2> <p>A stochastic process $X = { x(t): t\in T}$ is a collection of random variables. There are two elements:</p> <ul> <li>Time $t$: <ul> <li>discrete time ($T$ is a countably infinite set; under this case, we call <strong>Markov chain</strong>)</li> <li>continuous time (under this case, we call <strong>Markov process</strong>)</li> </ul> </li> <li>Space $\Omega$: <ul> <li>discrete space ($X_{t}$ comes from a countably infinite set)</li> <li>continuous space.</li> </ul> </li> </ul> <p>Markov chain is a <strong>discrete-time</strong> process for which the future behaviour, given the past and the present, only depends on the present and not on the past.</p> <p>Markov process is the <strong>continuous-time</strong> version of a Markov chain.</p> <blockquote> <p>Definition 1. [Markov chain] A discrete time stochastic process $ X_0, X_1, X_2, $. . . is a Markov chain if the following Markov property holds:</p> </blockquote> \[\small P(X_{t} = a_t \vert X_{t-1} = a_{t-1}, X_{t-2} = a_{t-2}, ..., X_0 = a_0) = P(X_{t} = a_t \vert X_{t-1} = a_{t-1}) = P_{a_{t-1}, a_{t}}\] <p><em>Remark 1</em>: This is time-homogeneous markov chain, for $\forall t$, for $\forall a_{t-1}, a_{t} \in \Omega$, the transition probability $P_{a_{t-1}, a_{t} }$ is the same.</p> <p><em>Remark 2</em>: In DDPM <d-cite key="ho2020denoising"></d-cite>, it is not a time-homogeneous chain, as the transition probability at t is obtained by a network(t).</p> <p>The state $X_{t}$ depends on the previous state $X_{t-1}$ but is independent of the particular history $X_{t-2}, X_{t-3},…$. This is called the <strong>Markov property</strong> or <strong>memoryless property</strong>.</p> <p>The Markov property does not imply that $X_{t}$ is independent of the random variables $X_{0}$, $X_{1}$,…, $X_{t-2}$; it just implies that <strong>any dependency of $X_{t}$ on the past is captured in the value of $X_{t-1}$</strong>.</p> <p>The Markov chain is <strong>uniquely</strong> defined by the one-step transition probability matrix $P$:</p> \[P = \begin{pmatrix} P_{0,0} &amp; P_{0, 1} &amp; \cdots &amp; P_{0, j} &amp; \cdots\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\ P_{i,0} &amp; P_{i, 1} &amp; \cdots &amp; P_{i, j} &amp; \cdots\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots&amp; \vdots\\ \end{pmatrix},\] <p>where $P_{i,j}$ is the probability of transition from state $i$ to state $j$. $P_{i,j} = P(X_{t} = j \vert X_{t-1} = i), i,j \in \Omega$. $\forall i, \sum_{j \geq 0} P_{i,j} = 1$.</p> <h2 id="classification-of-states">Classification of States</h2> <p>For simplicity, we assume that the state space $\Omega$ is finite. Below we introduce some concepts related to the classification of states.</p> <h3 id="communicating-class">Communicating class</h3> <blockquote> <p>Definition 2. [Communicating class] A state $j$ is reachable from state $i$ if there exists a positive integer $n$ such that $P_{i,j}^{(n)} &gt; 0$. We write $i \rightarrow j$. If $j$ is reachable from $i$, and $i$ is reachable from $j$, then the states $i$ and $j$ are said to <strong>communicate</strong>, denoted by $i \leftrightarrow j$. A communicating class $C$ is a <strong>maximal</strong> set of states that communicate with each other. <strong>No state in $C$ communicates with any state not in $C$.</strong></p> </blockquote> <h3 id="irreducible-markov-chain">Irreducible Markov chain</h3> <blockquote> <p>Definition 3. [Irreducibility] A Markov chain is <strong>irreducible</strong> if all states belong to <strong>one</strong> communicating class.</p> </blockquote> <p>This means that <strong>any state can be reached from any other state</strong>, <em>i.e.,</em> $\forall i, j \in \Omega$, $P_{i,j} &gt; 0$.</p> <blockquote> <p>Lemma 1. A finite Markov chain is irreducible if and only if its graph representation is a strongly connected graph.</p> </blockquote> <h3 id="transient-vs-recurrent-states">Transient vs recurrent states</h3> <p>Let $r_{i,j}^{t}$ denote the probability that the chain, starting at state $i$, <strong>the first time</strong> transition to state $j$ occurs at time $t$. That is, $ r_{i,j}^{t} = P(X_{t} = j, X_{s} \neq j, \forall 1 \leq s \leq t-1 \vert X_{0} = i). $</p> <blockquote> <p>Definition 4. [Transient and Recurrent States] A state is <strong>recurrent</strong> if $\sum_{t \geq 1} r_{i,i}^{t} = 1$ and it is <strong>transient</strong> if $\sum_{t \geq 1} r_{i,i}^{t} &lt; 1$. A Markov chain is recurrent if every state in the chain is recurrent.</p> </blockquote> <ul> <li> <p>If state i is recurrent then, once the chain visits that state, it will (with probability 1) eventually return to that state. Hence the chain will visit state $i$ over and over again, <strong>infinitely</strong> often.</p> </li> <li> <p>A transient state has the property that a Markov chain starting at this state returns to this state only <strong>finitely often</strong>, with probability 1.</p> </li> <li> <p>If one state in a communicating class is transient (respectively, recurrent) then all states in that class are transient (respectively, recurrent).</p> </li> </ul> <blockquote> <p>Definition 5. An irreducible Markov chain is called recurrent if at least one (equivalently, every) state in this chain is recurrent. An irreducible Markov chain is called transient if at least one (equivalently, every) state in this chain is transient.</p> </blockquote> <p>Let $\mu_{i} = \sum_{t \geq 1} t \cdot r_{i,i}^{t}$ denote the expected time to return to state $i$ when starting at state $i$.</p> <blockquote> <p>Definition 6. A state $i$ is <strong>positive recurrent</strong> if $\mu_{i} &lt; \infty$ and <strong>null recurrent</strong> if $\mu_{i} = \infty$.</p> </blockquote> <p>Here we give an example of a Markov chain that has null recurrent states. Consider the following markov chain whose states are the positive integers.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2023/markov-chains/image-480.webp 480w,/blog/2023/markov-chains/image-800.webp 800w,/blog/2023/markov-chains/image-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/blog/2023/markov-chains/image.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Starting at state 1, the probability of not having returned to state 1 within the first $t$ steps is \(\prod_{j=1}^{t} \frac{j}{j+1} = \frac{1}{t+1}.\) The probability of never returning to state 1 from state 1 is 0, and state 1 is recurrent. Thus, the probability of the first time transition to state $1$ occurs at time $t$ is \(r_{1,1}^{t} = \frac{1}{t} \cdot \frac{1}{t+1} = \frac{1}{t(t+1)}.\) The expected number of steps until the first return to state 1 when starting at state 1 is \(\mu_{1} = \sum_{t = 1}^{\infty} t \cdot r_{1,1}^{t} = \sum_{t = 1}^{\infty} \frac{1}{t+1} = \infty.\) State 1 is recurrent but null recurrent.</p> <blockquote> <p>Lemma 2. In a finite Markov chain:</p> <ol> <li>at least one state is recurrent; and</li> <li>all recurrent states are positive recurrent.</li> </ol> </blockquote> <p>Thus, all states of a finite, irreducible Markov chain are positive recurrent.</p> <h3 id="periodic-vs-aperiodic-states">Periodic vs aperiodic states</h3> <blockquote> <p>Definition 7. A state $j$ in a discrete time Markov chain is <strong>periodic</strong> if there exists an integer $k&gt;1$ such that $P(X_{t+s}= j \vert X_t = j) = 0$ unless $s$ is divisible by $k$. A discrete time Markov chain is periodic if any state in the chain is periodic. A state or chain that is not periodic is <strong>aperiodic</strong>.</p> </blockquote> <p>A state $i$ is periodic means that for $s = k, 2k, 3k,…$, $P(X_{t+s}= j \vert X_t = j) &gt; 0$ (<strong>N.B.: k &gt; 1</strong>).</p> <h3 id="ergodicity">Ergodicity</h3> <blockquote> <p>Definition 8. An <strong>aperiodic</strong>, <strong>positive recurrent</strong> state is an <strong>ergodic</strong> state. A Markov chain is ergodic if all its states are ergodic.</p> </blockquote> <blockquote> <p>Corollary 1. Any finite, irreducible, and aperiodic Markov chain is an ergodic chain.</p> </blockquote> <h3 id="stationary-distribution">Stationary distribution</h3> <p>Consider the two-state “broken printer” Markov chain:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/blog/2023/markov-chains/2023-07-22-11-00-52-480.webp 480w,/blog/2023/markov-chains/2023-07-22-11-00-52-800.webp 800w,/blog/2023/markov-chains/2023-07-22-11-00-52-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/blog/2023/markov-chains/2023-07-22-11-00-52.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Transition diagram for the two-state broken printer chain. </div> <p>There are two state (0 and 1) in this Markov chain, and assume that the initial distribution is:</p> \[P(X_0 = 0) = \frac{\beta}{\alpha+\beta}, P(X_0 = 1) = \frac{\alpha}{\alpha+\beta}.\] <p>Then, according to the transition probability matrix $P$, after one step, the distribution is</p> \[\small \begin{align*} P(X_1 = 0) &amp;= P(X_0 = 0)P(X_1 = 0 \vert X_0 = 0) + P(X_0 = 1)P(X_1 = 0 \vert X_0 = 1) \\ &amp;= \frac{\beta}{\alpha+\beta} \cdot (1-\alpha) + \frac{\alpha}{\alpha+\beta} \cdot \beta = \frac{\beta}{\alpha+\beta}, \\ P(X_1 = 1) &amp;= P(X_0 = 0)P(X_1 = 1 \vert X_0 = 0) + P(X_0 = 1)P(X_1 = 1 \vert X_0 = 1) \\ &amp;= \frac{\beta}{\alpha+\beta} \cdot \alpha + \frac{\alpha}{\alpha+\beta} \cdot (1-\beta) = \frac{\alpha}{\alpha+\beta}. \end{align*}\] <p>Apparently, the distribution of $X_1$ is the same as the initial distribution. Similarly, we can prove that the distribution of $X_t$ is the same as the initial distribution for any $t$. Here, $\pi = (\frac{\beta}{\alpha+\beta}, \frac{\alpha}{\alpha+\beta})$ is called <strong>stationary distribution</strong>.</p> <blockquote> <p>Definition 9. A probability distribution $\pi = (\pi_i)$, $\sum_{i \in \Omega} \pi_i = 1$(<strong>row vector</strong>) on the state space $\Omega$ is called a <strong>stationary distribution</strong> (or an equilibrium distribution) for the Markov chain with transition probability matrix $P$ if $\pi = \pi P$, equivalently, $\pi_j = \sum_{i \in \Omega}\pi_i P_{i,j}$ for all $j \in \Omega$.</p> </blockquote> <ul> <li> <p>One interpretation of the stationary distribution: if we started off a <strong>thousand</strong> Markov chains, choosing each starting position to be state $i$ with probability $\pi_i$, then(roughly) <strong>$1000 \pi_j$</strong> of them would be in state $j$ at any time in the future – but not necessarily the same ones each time.</p> </li> <li> <p>If a chain ever reaches a stationary distribution then it maintains that distribution for all future time, and thus a stationary distribution represents a steady state or an equilibrium in the chain’s behavior.</p> </li> </ul> <h3 id="finding-a-stationary-distribution">Finding a stationary distribution</h3> <p>Consider the following no-claims discount Markov chain with state space $\Omega = {1,2,3}$ and transition matrix:</p> \[P = \begin{pmatrix} \frac{1}{4} &amp; \frac{3}{4} &amp; 0\\ \frac{1}{4} &amp; 0 &amp; \frac{3}{4}\\ 0 &amp; \frac{1}{4} &amp; \frac{3}{4} \end{pmatrix}.\] <ul> <li> <p>Step 1: Assume $\pi = {\pi_1, \pi_2, \pi_3} $ is a stationary distribution. According to the definition 9 of stationary distribution, we need to solve the following equations:</p> \[\begin{align*} \pi_1 &amp;= \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2, \\ \pi_2 &amp;= \frac{3}{4}\pi_1 + \frac{1}{4}\pi_3, \\ \pi_3 &amp;= \frac{3}{4}\pi_2 + \frac{3}{4}\pi_3. \end{align*}\] <p>Adding the normalising condition $\pi_1 + \pi_2 + \pi_3 = 1$, we get four equations in three unknown parameters.</p> </li> <li> <p>Step 2: Choose one of the parameters, say $\pi_1$, and solve for the other two parameters in terms of $\pi_1$. We get</p> \[\pi_1 = \frac{1}{4}\pi_1 + \frac{1}{4}\pi_2 \Rightarrow \pi_2 = 3\pi_1, \pi_3 = 3\pi_2 = 9\pi_1.\] </li> <li> <p>Step 3: Combining with the normalising condition, we get</p> \[\pi_1 + 3\pi_1 + 9\pi_1 = 1 \Rightarrow \pi_1 = \frac{1}{13}, \pi_2 = \frac{3}{13}, \pi_3 = \frac{9}{13}.\] <p>Finally, we get the stationary distribution $\pi = (\frac{1}{13}, \frac{3}{13}, \frac{9}{13})$.</p> </li> </ul> <h3 id="existence-and-uniqueness">Existence and uniqueness</h3> <p>Given a Markov chaine, how can we know whether it has a stationary distribution? If it has, is it unique? At this part, we will answer these questions.</p> <p>Some notations:</p> <ul> <li> <p>Hitting time to hit the state $j$: $H_{j} = \min \{ t \in \{0, 1, 2,…\}: X_t = j\}$. Note that here we include time $t = 0$.</p> </li> <li> <p>Hitting probability to hit the state $j$ staring from state $i$: $h_{i,j} = P(X_t = j, \text{for some} \ t \geq 0 \vert X_0 = i) = P(H_{j} &lt; \infty \vert X_0 = i) = \sum_{t \geq 0} r_{i,j}^{t}.$</p> </li> </ul> <p>Note that this is different from $r_{i,j}^{t}$, which denotes the probability that the chain, starting at state $i$, the <strong>first</strong> time transition to state $j$ <strong>occurs at time $t$</strong>.</p> <p>We also have</p> \[h_{i,j} = \begin{cases} \sum_{k \in \Omega}P_{i,k}h_{k,j}, &amp; \text{if} \quad j \ne i, \\ 1, &amp; \text{if} \quad j = i. \end{cases}\] <ul> <li> <p><strong>Expected hitting time</strong>: $\eta_{i,j} = \mathbb{E}(H_{j} \vert X_0 = i) = \sum_{t \geq 0} t \cdot r_{i,j}^{t}$. The expected time until we hit state $j$ starting from state $i$. We also have</p> \[\eta_{i,j} = \begin{cases} 1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j}, &amp; \text{if} \quad j \ne i, \\ 0, &amp; \text{if} \quad j = i. \end{cases}\] <p>(For the first case, we add 1 because we need to consider the first step from state $i$ to state $k$.)</p> </li> <li> <strong>Return time</strong>: $M_i = \min \{ t \in \{1, 2,…\}: X_t = i\}$. It is different from $H_{i}$, as we exclude time $t = 0$. It is the first time that the chain returns to state $i$ after $t = 0$.</li> <li> <strong>Return probability</strong>: $m_{i} = P(X_t = i \ \text{for some} \ n \geq 1 \vert X_0 = i) = P(M_i &lt; \infty \vert X_0 = i) = \sum_{t&gt;1}r_{i,i}^{t}.$</li> <li> <strong>Expected return time</strong>: $\mu_{i} = E(M_i \vert X_0 = i) = \sum_{t \geq 1} t \cdot r_{i,i}^{t}$. The expected time until we return to state $i$ starting from state $i$. Therefore, we have the following equations: $ m_{i} = \sum_{j \in \Omega} P_{i,j}h_{j,i}, \mu_{i} = 1 + \sum_{j \in \Omega} P_{i,j}\eta_{j,i}. $</li> </ul> <h2 id="theorems-on-irreducible-markov-chains">Theorems on Irreducible Markov Chains</h2> <h3 id="existence-and-uniqueness-of-stationary-distribution">Existence and uniqueness of stationary distribution</h3> <blockquote> <p>Theorem 1. Consider an irreducible Markov chain (<strong>finite or infinite</strong>), (1) if it is <strong>positive recurrent</strong>, $\exists$ an unique stationary distribution $\pi$, such that $\pi_i = \frac{1}{\mu_{i}}$. (2) if it is <strong>null recurrent</strong> or <strong>transient</strong>, no stationary distribution exists.</p> </blockquote> <p><em>Remark</em>: If the chain is <strong>finite</strong> irreducible, it must be positive recurrent, thus it has an unique stationary distribution.</p> <p><em>Remark</em>: If the Markov chain is not irreducible, we can decompose the state space into several communicating classes. Then, we can consider each communicating class separately.</p> <ul> <li>If none of the classes are positive recurrent, then no stationary distribution exists.</li> <li>If exactly one of the classes is positive recurrent (and therefore closed), then there exists a unique stationary distribution, supported only on that closed class.</li> <li>If more the one of the classes are positive recurrent, then many stationary distributions will exist.</li> </ul> <p>Now, we give the proof of Theorem 1. We first prove that if a Markov chain is irreducible and positive recurrent, then there <strong>exists</strong> a stationary distribution. Next, we will prove the stationary distribution is <strong>unique</strong>. Since the second part with the null recurrent or transitive Markov chains is less important and more complicated, we will omit it. If you are interested in it, you can refer to the book <a href="https://www.statslab.cam.ac.uk/~james/Markov/" rel="external nofollow noopener" target="_blank">Markov Chains</a> by James Norris <d-cite key="norris1998markov"></d-cite>.</p> <p><em>Proof</em>. (1) Suppose that $(X_0, X_1 …)$ a recurrent Markov chain, which can be positive recurrent or null recurrent. Then we can desigh a stationary distribution as follows. (If we can desigh a stationary distribution, then it must be existed.)</p> <p>Let $\nu_i$ be the expected number of visits to $i$ before we return back to $k$,</p> \[\begin{align*} \nu_i &amp;= \mathbb{E}(\# \text{visits to $i$ before returning to } k \vert X_0 = k) \\ &amp;= \mathbb{E}\sum_{t=1}^{M_k} P(X_t = i \vert X_0 = k) \\ &amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1} P(X_t = i \vert X_0 = k) \end{align*}\] <p>The last equation holds because of $ P(X_0 = i \vert X_0 = k) = 0$ and $ P(X_{M_k} = i \vert X_0 = k) = 0$.</p> <p>If we want design a stationary distribution, it must statisfy $\pi P = \pi$ and $\sum_{i \in \Omega}\pi_i = 1$.</p> <p>(a) We first prove that $\nu P = \nu$.</p> \[\begin{align*} \sum_{i \in \Omega} \nu_i P_{i,j} &amp;= \mathbb{E}\sum_{i \in \Omega} \sum_{t = 0}^{M_k - 1} P(X_t = i, X_{t+1} = j \vert X_0 = k) \\ &amp;= \mathbb{E}\sum_{t = 0}^{M_k - 1} \sum_{i \in \Omega} P(X_t = i, X_{t+1} = j \vert X_0 = k) \\ &amp;= \mathbb{E} \sum_{t = 0}^{M_k - 1} P(X_{t+1} = j \vert X_0 = k) \\ &amp;= \mathbb{E} \sum_{t = 1}^{M_k } P(X_{t} = j \vert X_0 = k) \\ &amp;= \mathbb{E} \sum_{t = 0}^{M_k - 1} \nu_i \\ &amp;= \nu_j. \end{align*}\] <p>(b) Next, what we need to do is to normalize $\nu$ to get a stationary distribution. We have</p> \[\begin{align*} \sum_{i \in \Omega} \nu_i &amp;= \sum_{i \in \Omega} \mathbb{E} \sum_{t = 0}^{M_k - 1} P(X_t = i \vert X_0 = k) \\ &amp;=\mathbb{E} \sum_{t = 0}^{M_k - 1} \sum_{i \in \Omega} P(X_t = i \vert X_0 = k) \\ &amp;= \mathbb{E}(M_k \vert X_0 = i) \\ &amp;= \mu_k. \end{align*}\] <p>Thus, we can define $\pi_i = \nu_i/\mu_k$, $\pi = {\pi_i, i \in \Omega}$ is one of the stationary distribution.</p> <p>(2) Next, we prove that if a Markov chain is irreducible and positive recurrent, then the stationary distribution is <strong>unique</strong> and is given by $\pi_j = \frac{1}{\mu_j}$.</p> <p>Given a stationary distribution $\pi$, if we prove that for all $i$, $\pi_j == \frac{1}{\mu_j}$, then we prove that the stationary distribution is unique.</p> <p>Remember that the expected hitting time:</p> <p>\begin{equation} \eta_{i,j} = 1 + \sum_{k \in \Omega}P_{i,k}\eta_{k,j}, j \ne i \label{eq:1} \end{equation}</p> <p>We multiply both sides of Eq. \eqref{eq:1} by $\pi_i$ and sum over $i (i \ne j)$ to get</p> <p>\begin{equation} \sum_{i \ne j} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i \ne j} \sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j} \label{eq:2} \end{equation}</p> <p>Since $\eta_{j,j} = 0$, we can rewrite the above equation as</p> <p>\begin{equation} \sum_{i \in \Omega} \pi_i \eta_{i,j} = \sum_{i \ne j} \pi_i + \sum_{i \ne j} \sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j}. \label{eq:3} \end{equation}</p> <p>(The above equality lacks $j$, and we also want to design $\pi_j = 1/\mu_j$.)</p> <p>Remember that the expected return time:</p> \[\mu_{j} = 1 + \sum_{i \in \Omega} P_{j,i}\eta_{i,j}.\] <p>We multiply both sides of Eq. \eqref{eq:2} by $\pi_j$ to get</p> <p>\begin{equation} \pi_j \mu_{j} =\pi_j + \sum_{k \in \Omega} \pi_j P_{j,k}\eta_{k,j} \label{eq:4} \end{equation}</p> <p>Adding Eq. \eqref{eq:2} and Eq. \eqref{eq:4}, we get</p> \[\begin{align*} \sum_{i \in \Omega} \pi_i \eta_{i,j} + \pi_j \mu_{j} &amp;= \sum_{i \in \Omega} \pi_i + \sum_{i \in \Omega} \sum_{k \in \Omega} \pi_i P_{i,k}\eta_{k,j} \\ &amp;= 1 + \sum_{k \in \Omega} \sum_{i \in \Omega} \pi_i P_{i,k}\eta_{k,j} \\ &amp;= 1 + \sum_{k \in \Omega} \pi_k \eta_{k,j} \qquad (\text{since} \sum_{i \in \Omega} \pi_i P_{i,k} = \pi_k) \\ \end{align*}\] <p><strong>Since the Markov chain is irreducible and positive recurrent, that means all states belong to a communication class and the expected return time of each state is finite. Thus, the space $\Omega$ is a finite dimensional space.</strong></p> <p>We can substract $\sum_{k \in \Omega} \pi_k \eta_{k,j}$ and $\sum_{i \in \Omega} \pi_i \eta_{i,j} $ (equal) from both sides of the above equation to get $ \pi_j \mu_{j}=1, $ which means $\pi_j = 1/\mu_j$. Similarly, we can prove that $\pi_i = 1/\mu_i$ for all $i \in \Omega$.</p> <h3 id="limit-theorem">Limit theorem</h3> <blockquote> <p>Theorem 2. [Limit theorem] Consider an irreducible, aperiodic Markov chain (maybe infinite), we have $\lim\limits_{t \to \infty} P_{i,j}^{t} = \frac{1}{\mu_{j}}$. Spectially, (1) Suppose the Markov chain is positive recurrent. Then $\lim\limits_{t \to \infty} P_{i,j}^{t} = \pi_j = \frac{1}{\mu_{j}}$. (2) Suppose the Markov chain is null recurrent or transient. Then there is no limite probability.</p> </blockquote> <p>Three conditions for convergence to an equilibrium probability distribution: irreducibility, aperiodicity, and positive recurrence. The limit probability is</p> \[P = \begin{pmatrix} \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\ \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\ \pi_1 &amp; \pi_2 &amp; \cdots &amp; \pi_N\\ \end{pmatrix},\] <p>where each row is identical.</p> <h3 id="ergodic-theorem">Ergodic theorem</h3> <p>Define $V_{i,j}^{t} = \vert{ n &lt; t \vert X_n = j}\vert$. $V_{i,j}^{t}$ is the number of visits to state $j$ before time $t$ starting from state $i$. Then we can interpret $V_{i,j}^{t}/t$ as the proportion of time up to time $t$ spent in state $j$.</p> <blockquote> <p>Theorem 3. [Ergodic theorem] Consider an irreducible Markov chain, we have $\lim\limits_{t \to \infty} V_{i,j}^{t}/t = \frac{1}{\mu_{j}}$ <strong>almost surely</strong>. Spectially, (1) Suppose the Markov chain is positive recurrent. Then $\lim\limits_{t \to \infty} V_{i,j}^{t}/t = \pi_j = \frac{1}{\mu_{j}}$ <strong>almost surely</strong>. (2) Suppose the Markov chain is null recurrent or transient. Then $ V_{i,j}^{t}/t \to 0$ <strong>almost surely</strong> for all $j$.</p> </blockquote> <p>The term <strong>almost surely</strong> means that the convergence probability of the event is 1.</p> <h3 id="detailed-balance-condition">Detailed balance condition</h3> <blockquote> <p>Theorem 4. [Detailed balance condition]. Consider a finite, irreducible, and ergodic Markov chain with transition matrix $P$. If there are nonnegative numbers $\bar{\pi} = (\pi_0, \pi_1, …, \pi_n)$ such that $\sum_{i=0}^{n} \pi_i = 1$ and if, for any pair of states $i, j$, $ \pi_i P_{i,j} = \pi_{j} P_{j,i}, $ then $\bar{\pi}$ is the stationary distribution corresponding to $P$.</p> </blockquote> <p><em>Proof</em>. We have $ \sum_{i} \pi_i P_{i,j} = \sum_{i}\pi_{j} P_{j,i} = \pi_{j} $ Thus, $\bar{\pi} = \bar{\pi}P$. Since this is a finite, irreducible, and ergodic Markov chain, $\bar{\pi}$ must be the unique stationary distribution of the Markov chain.</p> <p><em>Remark</em>: Theorem 4 is a sufficient but not necessary condition.</p> <h2 id="further-reading">Further Reading</h2> <p>We have introduced the basic concepts of Markov chains, including the definition, classification of states, and theorems on irreducible Markov chains. If you are interested in more details, you can refer to the following materials:</p> <ul> <li>Mitzenmacher, M., &amp; Upfal, E. (2005). Probability and Computing. Cambridge University Press.</li> <li><a href="https://mpaldridge.github.io/math2750/S09-recurrence-transience.html" rel="external nofollow noopener" target="_blank">Recurrence and transience</a></li> <li><a href="https://mpaldridge.github.io/math2750/S07-classes.html" rel="external nofollow noopener" target="_blank">Class structure</a></li> <li><a href="https://mpaldridge.github.io/math2750/S10-stationary-distributions.html" rel="external nofollow noopener" target="_blank">Stationary distributions</a></li> <li>Stirzaker, David. <a href="https://www.ctanujit.org/uploads/2/5/3/9/25393293/_elementary_probability.pdf" rel="external nofollow noopener" target="_blank">Elementary Probability</a> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-07-12-markov-chains.bib"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-JGPDT2N8BY"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-JGPDT2N8BY");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>