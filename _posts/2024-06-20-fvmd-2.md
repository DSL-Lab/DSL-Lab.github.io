---
layout: distill
title: Video Evaluation Metrics 2/2 - Evaluating Motion Consistency by Fréchet Video Motion Distance (FVMD)
description: In this blog post, we introduce a promising new metric for video generative models, Fréchet Video Motion Distance (FVMD), which focuses on the motion consistency of generated videos.
tags: metrics video generative-models
giscus_comments: true
date: 2024-06-30
featured: true

authors:
  - name: Jiahe Liu
    url: "https://openreview.net/profile?id=~Jiahe_Liu1"
    affiliations:
      name: UBC
  - name: Qi Yan
    url: "https://qiyan98.github.io/"
    affiliations:
      name: UBC

bibliography: 2024-06-20-fvmd-2.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
  - name: Fréchet Video Motion Distance (FVMD)
  - subsections:
    - name: Video Key Points Tracking
    - name: Key Points Velocity and Acceleration Fields
    - name: Motion Feature
    - name: Visualizations
    - name: Fréchet Video Motion Distance
  - name: Experiments
  - subsections:
    - name: Sanity Check
    - name: Sensitivity Analysis
    - name: Quantitative Results
    - name: Human Study
  - name: Summary

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
# _styles: >
#   .fake-img {
#     background: #bbb;
#     border: 1px solid rgba(0, 0, 0, 0.1);
#     box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
#     margin-bottom: 12px;
#   }
#   .fake-img p {
#     font-family: monospace;
#     color: white;
#     text-align: left;
#     margin: 12px 0;
#     text-align: center;
#     font-size: 16px;
#   }
---



## Introduction

Recently, diffusion models have demonstrated remarkable capabilities in high-quality image generation. This advancement has been extended to the video domain, giving rise to text-to-video diffusion models, such as [Pika](https://pika.art/home), [Runway Gen-2](https://research.runwayml.com/gen2), and [Sora](https://openai.com/index/sora/) <d-cite key="videoworldsimulators2024"></d-cite>. 

Despite the rapid development of video generation models, research on evaluation metrics for video generation remains insufficient (see more discussion on our [blog](https://dsl-lab.github.io/blog/2024/fvmd-1/)). 
For example, FID-VID <d-cite key="balaji2019conditional"></d-cite> and FVD <d-cite key="unterthiner2018towards"></d-cite> are commonly used video metrics. FID-VID focuses on visual quality by comparing synthesized *frames* to real ones, ignoring motion quality. FVD adds temporal coherence by using features from a *pre-trained action recognition model*, Inflated 3D Convnet (I3D) <d-cite key="carreira2017quo"></d-cite>.
Recently, VBench <d-cite key="huang2023vbench"></d-cite> introduces a 16-dimensional evaluation suite for text-to-video generative models. However, VBench's protocols for temporal consistency, like temporal flickering and motion smoothness, favor videos with smooth or static movement, *neglecting high-quality videos with intense motion*, such as dancing and sports videos.

Simply put, there is a lack of metrics **specifically designed to evaluate the complex motion patterns in generated videos**. 
The Fréchet Video Motion Distance (FVMD) addresses this gap.

The code is available at [GitHub](https://github.com/DSL-Lab/FVMD-frechet-video-motion-distance).


## Fréchet Video Motion Distance (FVMD)
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/pipeline.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    The overall pipeline of the Fréchet Video Motion Distance (FVMD) that measures the discrepancy in motion features between generated videos and ground-truth videos.
</div>
The core idea of FVMD is to measure temporal motion consistency based on **the patterns of velocity and acceleration in video movements**. First, motion trajectories of key points are extracted using the pre-trained model PIPs++ <d-cite key="zheng2023pointodyssey"></d-cite>, and their velocity and acceleration are computed across frames. Motion features are then derived from the statistics of these vectors. Finally, the similarity between the motion features of generated and ground truth videos is measured using the Fréchet distance <d-cite key="dowson1982frechet"></d-cite>.

### Video Key Points Tracking

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/tracking_demo_1.gif" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/tracking_demo_2.gif" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Key point tracking results on the TikTok datasets <d-cite key="jafarian2022self"></d-cite> using PIPs++ <d-cite key="zheng2023pointodyssey"></d-cite>.
</div>

To construct video motion features, key point trajectories are first tracked across the video sequence using PIPs++.
For a set of $m$ generated videos, denoted as $\lbrace X^{(i)} \rbrace_{i=1}^m$, the tracking process begins by truncating longer videos into segments of $F$ frames with an overlap stride of $s$. 
For simplicity, segments from different videos are put together to form a single dataset $\lbrace x_{i} \rbrace_{i=1}^n$.
Then, $N$ evenly-distributed target points in a grid shape are queried on the initial frames <d-footnote>
For example, $F=16, s=1, N=400$ are used as default parameters to extract consecutive short segments.</d-footnote> and their trajectories are estimated across the video segments, resulting in a tensor $\hat{Y} \in \mathbb{R}^{F \times N \times 2}$. 


### Key Points Velocity and Acceleration Fields

FVMD proposes using the velocity and acceleration fields across frames to represent video motion patterns. 
The **velocity field** $\hat{V} \in \mathbb{R}^{F \times N \times 2}$ measures the first-order difference in key point positions between consecutive frames with zero-padding:

$$
  \hat{V} = \texttt{concat}(\boldsymbol{0}_{N\times 2}, \hat{Y}_{2:F} -  \hat{Y}_{1:F-1}) \in \mathbb{R}^{F \times N \times 2},
$$

The **acceleration field** $\hat{A} \in \mathbb{R}^{F \times N \times 2}$ is calculated by taking the first-order difference between the velocity fields in two consecutive frames, also with zero-padding:

$$
  \hat{A} = \texttt{concat}(\boldsymbol{0}_{N\times 2}, \hat{V}_{2:F} -  \hat{V}_{1:F-1}) \in \mathbb{R}^{F \times N \times 2}.
$$

### Motion Feature

To obtain compact motion features, the velocity and acceleration fields are further processed into **spatial and temporal statistical histograms**.

First, the *magnitude and angle* for each tracking point in the velocity and acceleration vector fields are computed respectively. Let $\rho(U)$ and $\phi(U)$ denote the magnitude and angle of a vector field $U$, where $U \in \mathbb{R}^{F \times N \times 2}$ and $U$ can be either $\hat{V}$ or $\hat{A}$.

$$
\begin{aligned}
  \rho(U)_{i, j} &= \sqrt{U_{i,j,1}^2 + U_{i,j,2}^2}, &\forall i \in [F], j \in [N], \\
  \phi(U)_{i, j} &= \left| \tanh^{-1}\left(\frac{U_{i, j,1}}{U_{i, j,2}}\right) \right|, &\forall i \in [F], j \in [N].
\end{aligned}
$$

Then, FVMD quantizes magnitudes and angles into discrete bins (8 for angles and 9 for magnitudes), which are then used to construct statistical histograms and extract motion features. It adopts **dense 1D histograms** <d-footnote>The 1D histogram approach is inspired by the HOG (Histogram of Oriented Gradients) approach <d-cite key="dalal2005histograms"></d-cite>, which counts occurrences of gradient orientation in localized portions of an image.</d-footnote> by aggregating magnitude values into 1D histograms corresponding to the quantized angles. 
Specifically, the $F$-frame video segments are divided into smaller volumes of size $f \times k \times k$, where $f$ is the number of frames and $k$ the number of tracking points.
Within each small volume, every tracking point's magnitude is summed into its corresponding angle bin, resulting in an 8-point histogram per volume. Eventually, the histograms from all volumes are combined to form the final motion feature <d-footnote>The shape of the dense 1D histogram is $ \lfloor \frac{F}{f} \rfloor \times \lfloor \frac{\sqrt{N}}{k} \rfloor \times \lfloor \frac{\sqrt{N}}{k} \rfloor \times 8$.</d-footnote>.

Dense 1D histograms are used for **both velocity and acceleration fields**, and the resulting features are concatenated to form a combined motion feature for computing similarity.
<!-- Empirical results identify this as the optimal configuration for the FVMD metric, making it the default choice. -->

<details>
  <summary>click here for 2D histogram construction</summary>
  FVMD also explores quantized 2D histograms but opts to use the dense 1D histograms for the default configuration due to their superior performance.
  In this approach, the corresponding vector fields of each volume are aggregated to form a 2D histogram, where $x$ and $y$ coordinates represent magnitudes and angles, respectively. The 2D histograms from all volumes are then concatenated and flattened into a vector to serve as the motion feature. The shape of the quantized 2D histogram is $ \lfloor \frac{F}{f} \rfloor \times \lfloor \frac{\sqrt{N}}{k} \rfloor \times \lfloor \frac{\sqrt{N}}{k} \rfloor \times 72$, where the number 72 is derived from 8 discrete bins for angle and 9 bins for magnitude.
</details>

### Visualizations
If two videos are of very different quality, their histograms should look very *different* to serve as a discriminative motion feature. Let's take a look at what they look like for the videos in real life.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/gt.gif" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/disco.gif" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/anyone.gif" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/gt_tracking.gif" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/disco_tracking.gif" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/anyone_tracking.gif" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Raw videos and tracking results on the TikTok datasets <d-cite key="jafarian2022self"></d-cite>.
    Left: Ground-truth video.
    Middle and right: Generated videos for the same scene of worse (middle) and better (right) quality, respectively.
</div>

Above, we show three pieces of video from the TikTok dataset <d-cite key="jafarian2022self"></d-cite> with very different visual qualities for the same scene. One can easily spot their differences in motion patterns. Next, we show the 1D histograms based on the velocity field of the videos.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/gt_v_1d.png" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/disco_v_1d.png" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/anyone_v_1d.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Dense 1D histograms for the velocity fields of the videos.
    Left: Ground-truth video. Middle and right: Generated videos for the same scene of worse (middle) and better (right) quality, respectively.
</div>

The low-quality video has more abrupt motion changes, resulting in a substantially greater number of large-angle velocity vectors.
Therefore, the **higher-quality video (right) has a motion pattern closer to the ground-truth video (left) than the lower-quality video (middle)**.
This is exactly what we want to observe in the motion features! These features can capture the motion patterns effectively and distinguish between videos of different qualities.


<details>
  <summary>click here for 2D histogram result</summary>
  <div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/gt_v_2d.png" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/disco_v_2d.png" class="img-fluid rounded z-depth-1" %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/anyone_v_2d.png" class="img-fluid rounded z-depth-1" %}
    </div>
  </div>
  <div class="caption">
    Dense 2D histograms for the velocity fields of the videos.
    Left: ground-truth video. Middle and right: Generated videos of worse and better quality, respectively.
  </div>
  We can observe similar patterns in the 2D histograms. The higher-quality video (right) has a motion pattern closer to the ground-truth video (left) than the lower-quality video (middle). The unnatural jittering and unsmooth motion in the lower-quality video lead to more frequent large-magnitude velocity vectors, as captured by the 2D histograms.
</details>



### Fréchet Video Motion Distance
After extracting motion features from video segments of generated and ground-truth video sets, FVMD measures their similarity using the **Fréchet distance** <d-cite key="dowson1982frechet"></d-cite>, which explains the name *Fréchet Video Motion Distance (FVMD)*.
To make the computation tractable, multi-dimensional Gaussian distributions are used to represent the motion features, following previous works <d-cite key="heusel2017gans"></d-cite>.
Let $\mu_{\text{gen}}$ and $\mu_{\text{data}}$ be the mean vectors, and $\Sigma_{\text{gen}}$ and $\Sigma_{\text{data}}$ be the covariance matrices of the generated and ground-truth videos, respectively. The FVMD is defined as:

$$
    d_F = ||\mu_{\text{data}}-\mu_{\text{gen}}{||}_2^2 + \mathrm{tr}\left(\Sigma_{\text{data}} + \Sigma_{\text{gen}} -2(\Sigma_{\text{data}}\Sigma_{\text{gen}})^{\frac{1}{2}}\right)
$$

## Experiments
The ultimate aim of a video evaluation metric is to align with human perception. To validate the effectiveness of FVMD, a series of experiments is conducted in the paper, including **sanity check**, **sensitivity analysis**, and **quantitative comparison** with existing metrics. **Large-scale human studies** are also performed to compare the performance of FVMD with other metrics.

### Sanity Check
To verify the efficacy of the extracted motion features in representing motion patterns, a sanity check is performed in the FVMD paper. Motion features based on velocity, acceleration, and their combination are used to compare videos from the same dataset and different datasets.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/sanity_check.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    <!-- Dense 1D histograms based on velocity, acceleration, and their combination are used to construct FVMD metrics.  -->
    As sample size increases, same-dataset discrepancies (BAIR <d-cite key="ebert2017self"></d-cite> vs BAIR) converge to zero, while cross-dataset discrepancies (TIKTOK  <d-cite key="jafarian2022self"></d-cite> vs BAIR) remain large, verifying the soundness of the FVMD metric.
</div>
When measuring the FVMD of **two subsets from the same dataset**, it **converges to zero as the sample size increases**, confirming that the motion distribution within the same dataset is consistent. Conversely, the FVMD **remains higher for subsets from different datasets**, showing that their motion patterns exhibit a larger gap compared to those within the same dataset.

### Sensitivity Analysis
Moreover, a sensitivity analysis is conducted to evaluate if the proposed metric can effectively detect temporal inconsistencies in generated videos, *i.e.*, being **numerically sensitive to temporal noises**. To this end, artificially-made temporal noises are injected to the TikTok dancing dataset <d-cite key="jafarian2022self"></d-cite> and FVMD scores are computed to assess its sensitivity to data corruption.
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/sensitivity_ana.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    The FVMD scores in the presence of various temporal noises are presented.
</div>

Across the four types of temporal noises injected into the dataset <d-footnote>
  There are four types of temporary noises in the FVMD paper: 1) local swap: swapping a fraction of consecutive frames in the video sequence, 2) global swap: swapping a fraction of frames in the video sequence with randomly chosen frames, 3) interleaving: weaving the sequence of frames corresponding to multiple different videos to obtain new videos, 4) switching: jumping from one video to another video.
</d-footnote>, **FVMD based on combined velocity and acceleration features** demonstrates the most reliable performance. It shows a strong negative correlation with noise level, indicating FVMD's sensitivity to temporal noise and its effectiveness in detecting temporal inconsistencies in generated videos.

### Quantitative Results

Further, FVMD provides a quantitative comparison of various video evaluation metrics on TikTok dataset <d-cite key="jafarian2022self"></d-cite>. 
Fifty videos are generated using different checkpoints named (a) through (e) <d-footnote>The video samples are reproduced from the following models: (a) is from Magic Animate <d-cite key="xu2023magicanimate"></d-cite>; (b), (c), and (e) are from Animate Anyone <d-cite key="hu2023animate"></d-cite>, each with different training hyperparameters; and (d) is from DisCo <d-cite key="wang2023disco"></d-cite>.</d-footnote> and their performance is measured using the FVD <d-cite key="unterthiner2018towards"></d-cite>, FID-VID <d-cite key="heusel2017gans"></d-cite>, VBench <d-cite key="huang2023vbench"></d-cite>, and FVMD metrics. 
Note that the models (a) to (e) are sorted based on human ratings collected through a user study, from worse to better visual quality (model (e) has the best visual quality and model (a) has the worst). This allows for a comparison of **how well the evaluation metrics align with human judgments**.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="blog/2024/fvmd/FVMD.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=true %}
    </div>
</div>
<div class="caption">
    Video samples created by various video generative models trained on the TikTok dataset <d-cite key="jafarian2022self"></d-cite> are shown to compare the fidelity of different evaluation metrics.
</div>

| **Metrics**     | **Model (a)**  | **Model (b)**  | **Model (c)**  | **Model (d)**  | **Model (e)**  | **Human Corr.↑**|
|-----------------|----------------|----------------|----------------|----------------|----------------|---------------------------------|
| FID↓            | 73.20 (3rd)        | 79.35 (4th)        | 63.15 (2nd)        | 89.57 (5th)        | 18.94 (1st)        | 0.3                             |
| FVD↓            | 405.26 (4th)       | 468.50 (5th)       | 247.37 (2nd)       | 358.17 (3rd)       | 147.90 (1st)       | 0.8                             |
| VBench↑         | 0.7430 (5th)       | 0.7556 (4th)       | 0.7841 (2nd)       | 0.7711 (3rd)       | 0.8244 (1st)       | 0.9                             |
| FVMD↓           | 7765.91 (5th)      | 3178.80 (4th)      | 2376.00 (3rd)      | 1677.84 (2nd)      | 926.55 (1st)       | **1.0**                             |

FVMD ranks the models correctly in line with human ratings and has **the highest correlation to human perceptions**. Moreover, FVMD provides **distinct scores for video samples of different quality**, showing a clearer separation between models.

### Human Study

In the paper, large-scale human studies are conducted to validate that the proposed FVMD metric aligns with human perceptions. Three different human pose-guided generative models are fine-tuned: DisCo <d-cite key="wang2023disco"></d-cite>, Animate Anyone <d-cite key="xu2023magicanimate"></d-cite>, and Magic Animate <d-cite key="xu2023magicanimate"></d-cite>. These models, with distinct architectures and hyper-parameter settings, yield over 300 checkpoints with varying sample qualities. 
Users are then asked to compare samples from each pair of models to form a ground-truth user score. 
All checkpoints are also automatically evaluated using the FVMD metric, and the results are compared with FID-VID <d-cite key="heusel2017gans"></d-cite>, FVD <d-cite key="unterthiner2018towards"></d-cite>, SSIM <d-cite key="wang2004image"></d-cite>, PSNR <d-cite key="wang2004image"></d-cite>, and VBench <d-cite key="huang2023vbench"></d-cite>. 
**The correlation between the scores given by each metric and the ground-truth user scores is calculated to further assess the performance of each metric.**

Following the model selection strategy in <d-cite key="unterthiner2018towards"></d-cite>, two settings for the human studies are designed. The first setup is **One Metric Equal**. In this approach, a group of models with nearly identical scores based on a selected metric is identified. Namely, the selected models' generated samples are considered to have similar visual quality compared to the reference data, according to the selected metric. This setup investigates whether the other metrics and human raters can effectively differentiate between these models. 

The second setting, **One Metric Diverse**, evaluates the agreement among different metrics when a single metric provides a clear ranking of the performances of the considered video generative models. Specifically, model checkpoints whose samples can be clearly differentiated according to the given metric are selected to test the consistency between this metric, other metrics, and human raters.


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/human_study_eql.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Table 1: Pearson correlation for the One Metric Equal experiments.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="blog/2024/fvmd/human_study_div.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Table 2: Pearson correlation for One Metric Diverse experiments.
</div>

The Pearson correlations range in [-1, 1], with values closer to -1 or 1 indicating stronger negative or positive correlation, respectively. The agreement rate among raters is reported as a percentage from 0 to 1. A higher agreement rate indicates a stronger consensus among human raters and higher confidence in the ground-truth user scores. The correlation is higher-the-better for all metrics in both **One Metric Equal** and **One Metric Diverse** settings. Overall, FVMD demonstrates the strongest capability to distinguish videos when other metrics fall short.

## Summary
In this blog, we give a brief summary of the recently-proposed **Fréchet Video Motion Distance (FVMD)** metric and its advantages over existing metrics.
FVMD is designed to evaluate the motion consistency of generated videos by comparing the discrepancies of velocity and acceleration patterns between generated and ground-truth videos.
The metric is validated through a series of experiments, including a sanity check, sensitivity analysis, quantitative comparison, and large-scale human studies. The results show that FVMD outperforms existing metrics in many aspects, such as better alignment with human judgment and a stronger capability to distinguish videos of different quality.
