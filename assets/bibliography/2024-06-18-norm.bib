@misc{lipman_flow_2023,
	title = {Flow Matching for Generative Modeling},
	url = {http://arxiv.org/abs/2210.02747},
	doi = {10.48550/arXiv.2210.02747},
	abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows ({CNFs}), allowing us to train {CNFs} at unprecedented scale. Specifically, we present the notion of Flow Matching ({FM}), a simulation-free approach for training {CNFs} based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing {FM} with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training {CNFs} with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport ({OT}) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training {CNFs} using Flow Matching on {ImageNet} leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical {ODE} solvers.},
	number = {{arXiv}:2210.02747},
	publisher = {{arXiv}},
	author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
	urldate = {2024-07-05},
	date = {2023-02-08},
	eprinttype = {arxiv},
	eprint = {2210.02747 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\robin\\Zotero\\storage\\Z965SN3B\\2210.html:text/html},
}

@misc{behrmann_invertible_2019,
	title = {Invertible Residual Networks},
	url = {http://arxiv.org/abs/1811.00995},
	doi = {10.48550/arXiv.1811.00995},
	abstract = {We show that standard {ResNet} architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible {ResNets} define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible {ResNets} perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.},
	number = {{arXiv}:1811.00995},
	publisher = {{arXiv}},
	author = {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, Jörn-Henrik},
	urldate = {2024-07-17},
	date = {2019-05-18},
	eprinttype = {arxiv},
	eprint = {1811.00995 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\robin\\Zotero\\storage\\R73WZ69F\\1811.html:text/html},
}

@misc{chen_residual_2020,
	title = {Residual Flows for Invertible Generative Modeling},
	url = {http://arxiv.org/abs/1906.02735},
	doi = {10.48550/arXiv.1906.02735},
	abstract = {Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estimate of the log density using a "Russian roulette" estimator, and reduce the memory required during training by using an alternative infinite series for the gradient. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid derivative saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.},
	number = {{arXiv}:1906.02735},
	publisher = {{arXiv}},
	author = {Chen, Ricky T. Q. and Behrmann, Jens and Duvenaud, David and Jacobsen, Jörn-Henrik},
	urldate = {2024-07-17},
	date = {2020-07-23},
	eprinttype = {arxiv},
	eprint = {1906.02735 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\robin\\Zotero\\storage\\INCJEVDR\\1906.html:text/html},
}

@misc{kingma_glow_2018,
	title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
	url = {http://arxiv.org/abs/1807.03039},
	doi = {10.48550/arXiv.1807.03039},
	shorttitle = {Glow},
	abstract = {Flow-based generative models (Dinh et al., 2014) are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using an invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood on standard benchmarks. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient realistic-looking synthesis and manipulation of large images. The code for our model is available at https://github.com/openai/glow},
	number = {{arXiv}:1807.03039},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Dhariwal, Prafulla},
	urldate = {2024-08-11},
	date = {2018-07-10},
	eprinttype = {arxiv},
	eprint = {1807.03039 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\robin\\Zotero\\storage\\4NGNMZ86\\1807.html:text/html},
}

@misc{dinh_nice_2015,
	title = {{NICE}: Non-linear Independent Components Estimation},
	url = {http://arxiv.org/abs/1410.8516},
	doi = {10.48550/arXiv.1410.8516},
	shorttitle = {{NICE}},
	abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation ({NICE}). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
	number = {{arXiv}:1410.8516},
	publisher = {{arXiv}},
	author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
	urldate = {2024-08-11},
	date = {2015-04-10},
	eprinttype = {arxiv},
	eprint = {1410.8516 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\robin\\Zotero\\storage\\I97AFBCF\\1410.html:text/html},
}

@misc{dinh_density_2017,
	title = {Density estimation using Real {NVP}},
	url = {http://arxiv.org/abs/1605.08803},
	doi = {10.48550/arXiv.1605.08803},
	abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real {NVP}) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
	number = {{arXiv}:1605.08803},
	publisher = {{arXiv}},
	author = {Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
	urldate = {2024-08-11},
	date = {2017-02-27},
	eprinttype = {arxiv},
	eprint = {1605.08803 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\robin\\Zotero\\storage\\4EWH66A4\\1605.html:text/html},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-Encoding Variational Bayes},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	number = {{arXiv}:1312.6114},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2024-08-11},
	date = {2022-12-10},
	eprinttype = {arxiv},
	eprint = {1312.6114 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\robin\\Zotero\\storage\\M7VE5BGC\\1312.html:text/html},
}

@article{goodfellow_generative_2020,
	title = {Generative adversarial networks},
	volume = {63},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3422622},
	doi = {10.1145/3422622},
	abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks ({GANs}) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but {GANs} are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). {GANs} have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
	pages = {139--144},
	number = {11},
	journaltitle = {Commun. {ACM}},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2024-08-11},
	date = {2020-10-22},
}

@misc{song_score-based_2021,
	title = {Score-Based Generative Modeling through Stochastic Differential Equations},
	url = {http://arxiv.org/abs/2011.13456},
	doi = {10.48550/arXiv.2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation ({SDE}) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time {SDE} that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time {SDE} depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical {SDE} solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time {SDE}. We also derive an equivalent neural {ODE} that samples from the same distribution as the {SDE}, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on {CIFAR}-10 with an Inception score of 9.89 and {FID} of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
	number = {{arXiv}:2011.13456},
	publisher = {{arXiv}},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	urldate = {2024-08-11},
	date = {2021-02-10},
	eprinttype = {arxiv},
	eprint = {2011.13456 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\robin\\Zotero\\storage\\YXYVPUKI\\2011.html:text/html},
}

@online{weng_flow-based_2018,
	title = {Flow-based Deep Generative Models},
	url = {https://lilianweng.github.io/posts/2018-10-13-flow-models/},
	abstract = {So far, I’ve written about two types of generative models, {GAN} and {VAE}. Neither of them explicitly learns the probability density function of real data, \$p({\textbackslash}mathbf\{x\})\$ (where \${\textbackslash}mathbf\{x\} {\textbackslash}in {\textbackslash}mathcal\{D\}\$) — because it is really hard! Taking the generative model with latent variables as an example, \$p({\textbackslash}mathbf\{x\}) = {\textbackslash}int p({\textbackslash}mathbf\{x\}{\textbackslash}vert{\textbackslash}mathbf\{z\})p({\textbackslash}mathbf\{z\})d{\textbackslash}mathbf\{z\}\$ can hardly be calculated as it is intractable to go through all possible values of the latent code \${\textbackslash}mathbf\{z\}\$.},
	author = {Weng, Lilian},
	urldate = {2024-08-11},
	date = {2018-10-13},
	langid = {english},
	note = {Section: posts},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising Diffusion Probabilistic Models},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional {CIFAR}10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art {FID} score of 3.17. On 256x256 {LSUN}, we obtain sample quality similar to {ProgressiveGAN}.},
	pages = {6840--6851},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	urldate = {2024-08-11},
	date = {2020},
}

@video{lipman_flow_nodate,
	title = {Flow Matching: Simplifying and Generalizing Diffusion Models {\textbar} Yaron Lipman},
	url = {https://www.youtube.com/watch?v=5ZSwYogAxYg},
	shorttitle = {Flow Matching},
	abstract = {Unlocking the Future of Drug Discovery with Generative {AI}!In our third talk, Yaron Lipman (Weizmann Institute of Science, Meta) will give us an overview of F...},
	author = {Lipman, Yaron},
	urldate = {2024-08-11},
	date = {2024-08-11},
	langid = {english},
}

@online{noauthor_222_nodate,
	title = {22.2 - Change-of-Variable Technique {\textbar} {STAT} 414},
	url = {https://online.stat.psu.edu/stat414/lesson/22/22.2},
	urldate = {2024-08-11},
	langid = {english},
}

@video{marcus_normalizing_nodate,
	title = {Normalizing Flows and Invertible Neural Networks in Computer Vision ({CVPR} 2021 Tutorial)},
	url = {https://www.youtube.com/watch?v=8XufsgG066A},
	abstract = {{CVPR} 2021 Tutorial on Normalizing Flows and Invertible Neural Networks in Computer {VisionLooking} for more about normalizing flows?  Maybe start with these re...},
	author = {Marcus, Brubaker},
	urldate = {2024-08-11},
	langid = {english},
}