@misc{lipman_flow_2023,
	title = {Flow Matching for Generative Modeling},
	url = {http://arxiv.org/abs/2210.02747},
	doi = {10.48550/arXiv.2210.02747},
	abstract = {We introduce a new paradigm for generative modeling built on Continuous Normalizing Flows ({CNFs}), allowing us to train {CNFs} at unprecedented scale. Specifically, we present the notion of Flow Matching ({FM}), a simulation-free approach for training {CNFs} based on regressing vector fields of fixed conditional probability paths. Flow Matching is compatible with a general family of Gaussian probability paths for transforming between noise and data samples -- which subsumes existing diffusion paths as specific instances. Interestingly, we find that employing {FM} with diffusion paths results in a more robust and stable alternative for training diffusion models. Furthermore, Flow Matching opens the door to training {CNFs} with other, non-diffusion probability paths. An instance of particular interest is using Optimal Transport ({OT}) displacement interpolation to define the conditional probability paths. These paths are more efficient than diffusion paths, provide faster training and sampling, and result in better generalization. Training {CNFs} using Flow Matching on {ImageNet} leads to consistently better performance than alternative diffusion-based methods in terms of both likelihood and sample quality, and allows fast and reliable sample generation using off-the-shelf numerical {ODE} solvers.},
	number = {{arXiv}:2210.02747},
	publisher = {{arXiv}},
	author = {Lipman, Yaron and Chen, Ricky T. Q. and Ben-Hamu, Heli and Nickel, Maximilian and Le, Matt},
	urldate = {2024-07-05},
	date = {2023-02-08},
	eprinttype = {arxiv},
	eprint = {2210.02747 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\robin\\Zotero\\storage\\Z965SN3B\\2210.html:text/html},
}

@misc{chen_neural_2019,
	title = {Neural Ordinary Differential Equations},
	url = {http://arxiv.org/abs/1806.07366},
	doi = {10.48550/arXiv.1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any {ODE} solver, without access to its internal operations. This allows end-to-end training of {ODEs} within larger models.},
	number = {{arXiv}:1806.07366},
	publisher = {{arXiv}},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	urldate = {2024-07-17},
	date = {2019-12-13},
	eprinttype = {arxiv},
	eprint = {1806.07366 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\robin\\Zotero\\storage\\CF8G6BFT\\1806.html:text/html},
}

@misc{grathwohl_ffjord_2018,
	title = {{FFJORD}: Free-form Continuous Dynamics for Scalable Reversible Generative Models},
	url = {http://arxiv.org/abs/1810.01367},
	doi = {10.48550/arXiv.1810.01367},
	shorttitle = {{FFJORD}},
	abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
	number = {{arXiv}:1810.01367},
	publisher = {{arXiv}},
	author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	urldate = {2024-07-17},
	date = {2018-10-22},
	eprinttype = {arxiv},
	eprint = {1810.01367 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\robin\\Zotero\\storage\\K6KNG2ZG\\1810.html:text/html},
}

@online{huang_how_nodate,
	title = {How I Understand Flow Matching - {YouTube}},
	url = {https://www.youtube.com/watch?v=DDq_pIfHqLs},
	author = {Huang, Jia-Bin},
	urldate = {2024-08-11},
}

@online{m_tomczak_flow_nodate,
	title = {Flow Matching: Matching flows instead of scores},
	url = {https://jmtomczak.github.io/blog/18/18_fm.html},
	author = {M. Tomczak, Jakub},
	urldate = {2024-08-11},
	file = {18_flow_matching:C\:\\Users\\robin\\Zotero\\storage\\KJABUQZF\\18_fm.html:text/html},
}
